{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Dustin Barnes","n":0.707},"1":{"v":"\n# Dustin Barnes\n\nThis is my digital garden. It's not monetized, there's no analytics. It's probably not great at SEO. But it's simple, it's text-based, and it's interconnected. \n\nA few things of note: \n\n- My main GitHub profile is https://github.com/dustinbarnes\n- This blog is built from https://github.com/dustinbarnes/blog. \n  - Notice the GitHub Actions powering the publish process: https://github.com/dustinbarnes/blog/blob/main/.github/workflows/publish.yml\n- My dotfiles repo: https://github.com/dustinbarnes/dotfiles\n","n":0.129}}},{"i":2,"$":{"0":{"v":"Tech","n":1}}},{"i":3,"$":{"0":{"v":"Management","n":1}}},{"i":4,"$":{"0":{"v":"Prioritization","n":1},"1":{"v":"\nPrioritization is a way to determine which activities are most important. \n\nImportancy considerations:\n- ROI\n- Time-to-market\n- Roadmap completion\n- Product vision\n","n":0.229}}},{"i":5,"$":{"0":{"v":"Stack Rank","n":0.707},"1":{"v":"\nStack-ranking is a technique where the items to be prioritized are put in a ranked order from most to least important.\n\n- **There can only be one number one.** So, you will avoid a common pitfall where everything becomes very high priority.\n- **It is often more accurate, less confusing.** You prioritize each item relative to all other items, which is simpler and clearer. \n","n":0.126}}},{"i":6,"$":{"0":{"v":"Moscow","n":1},"1":{"v":"\nThe MoSCoW model has nothing to do with Russia’s capital. Rather, it takes its name from the letters that make up its criteria: Must Have, Should Have, Could Have and Won’t Have.\n\n- **Must Have** — If you would have to cancel your release if you couldn’t include it, then it’s a Must Have. Must-Have items are those that you guarantee to deliver because you can’t deliver without, or it would be illegal or unsafe without. If there’s **any** way to deliver without it — a workaround, for example — then it should be downgraded to Should Have or Could Have. That doesn’t mean it won’t be delivered. It means that that delivery is not guaranteed.\n\n- **Should Have** — Should Have items are important, but not absolutely vital to the success of your release. They can be painful to leave out, and may have an impact on your product, but they don’t affect minimum viability of your product.\n\n- **Could Have** — Could Have items are those that are wanted or desirable but are less important than a Should Have item. Leaving them out will cause less pain than a Should Have item.  \n\n- **Won’t Have** — Won’t Have user stories are those in which everyone has agreed not to deliver this time around. You may keep it in the backlog for later, if or when it becomes necessary to do so.\n","n":0.066}}},{"i":7,"$":{"0":{"v":"Kano","n":1},"1":{"v":"\nFeatures are categorized according to needs and expectations of customers. There are some variations, however the original Kano method classifies items using five thresholds: Must-be, Attractive, One-Dimensional, Indifferent, and Reverse.\n\n- **Must-Be** — These are expected by your customers. They are features that will not WOW them. They must be included in your product, and are often taken for granted. Often called 'Table Stakes'\n\n- **Attractive** — These make users happy when they’re there, but don’t disappoint them when they’re not.\n\n- **One-Dimensional** — These are features that make users happy when they’re there, unhappy when they’re not.\n\n- **Indifferent** — These have no impact on customer satisfaction levels. For example, refactoring parts of your code so that it is easier to read and understand. There is no direct value to the customer, but it will make it easier for you to maintain in the future.\n\n- **Reverse** — These make users unhappy when they’re there, happy when they’re not. For example, you might implement high-security features requiring an extra step to login. However, if customers do not value enhanced security, they will become dissatisfied with the extra step.","n":0.074}}},{"i":8,"$":{"0":{"v":"Cost of Delay","n":0.577},"1":{"v":"> \"If you are going to quantify one thing, quantify the cost of delay\"  \n> -- Donald Reinertsen, lean thought leader\n\nTo quantify the cost of delay is to answer the question: “What will be the cost per time unit if we delayed delivery?” You can then compare your answer with the estimate to deliver the feature with the highest cost of delay, and is cheapest to do, first. This is called Weighted Shortest Job First (WSJF) prioritization.\n\nCost of delay is not necessarily measured in terms of dollars. There are many ways to assess value and cost. Reputation or story points are two examples.\n\nQuantifying cost of delay for any given feature can be challenging. Those who are good at it try to consider how cost of delay changes over time. They commonly use standards, such as the following.\n\n- **Linear** — For every day we do not deliver, we lose some money. A common example of a linear cost of delay is money lost due to competitors already having a feature that you don’t.\n\n- **Fixed date** — If we don’t deliver by a certain date, it’s too late. An example: let’s imagine you’re making New Year cards for 2019. If you don’t deliver them before the end of 2018, the cost of delay is very high. In fact, delivering afterward, in January or February, makes no difference — it is too late!\n\n- **Intangible** — We can delay for now at minimal cost, but eventually it could become expensive. A good example is the cost of delay for fixing a few bugs or refactoring your code. You can skip today, but over time it will make other improvements more expensive and can cause the cost of delay to increase exponentially.\n\n- **Expedite** — It must be done immediately or the cost of delay will grow radically. An example of an expedite feature is a severe bug that renders your product useless to all your customers.\n\nThe cost of delay categorization, when compared against cost of implementation, will often give you a good idea of what should be done first.","n":0.054}}},{"i":9,"$":{"0":{"v":"Consensus","n":1},"1":{"v":"Consensus means coming to an agreement. Creating consensus in a team setting means finding a proposal acceptable enough that all team members can support it, with no member opposing it. \n\nConsensus includes:\n\n- pooling opinions;\n- listening effectively;\n- discussing ideas and differences;\n- not getting all you want; and\n- coming to an agreement that everyone \"can live with.\"\n\nConsensus is not:\n\n- a unanimous vote;\n- majority or minority rule;\n- one person rule; or\n- bargaining.\n","n":0.12}}},{"i":10,"$":{"0":{"v":"Voting","n":1},"1":{"v":"Multi-voting can reduce a list containing a large number of items to a manageable few. \n\n- Each person gets some number of votes\n- Members vote on each idea they think are best\n    - Only one vote per item per person\n\nOnce everyone has voted, re-order the items with the most votes first\n\nThis process can then repeat again, if fewer items are needed.\n","n":0.128}}},{"i":11,"$":{"0":{"v":"Fist of Five","n":0.577},"1":{"v":"Fist of Five is a means of establishing consensus in a group. \n\nEveryone is asked to show their agreement with a statement by holding up a number of fingers.\n\nNumbers generally mean:\n\n- Lack of consensus:\n    - 0: I'll block this.\n    - 1: I see major issues we need to resolve now.\n    - 2: I see minor issues we need to resolve now.\n- Consensus:\n    - 3: I see minor issues we can resolve later.\n    - 4: I'm okay with this as it is.\n    - 5: I love this, and will champion it.\n\n\n","n":0.105}}},{"i":12,"$":{"0":{"v":"Brainstorming","n":1},"1":{"v":"Brainstorming is a simple and effective way to get ideas out of people. \n\n- Facilitator defines problem/topic\n- Each team member suggests ideas\n- Ideas written on board\n- No criticizing/evaluating ideas yet\n\nAfter this is done, the list needs to be refined: \n\n- Combine ideas\n- Gather explanations\n\nThen it's time for [[tech.management.prioritization]]!\n","n":0.144}}},{"i":13,"$":{"0":{"v":"Config","n":1}}},{"i":14,"$":{"0":{"v":"Cuelang","n":1},"1":{"v":"\nhttps://cuelang.org/\n\nIntro cf site:\n\n> CUE is an open-source data validation language and inference engine with its roots in logic programming. Although the language is not a general-purpose programming language, it has many applications, such as data validation, data templating, configuration, querying, code generation and even scripting. The inference engine can be used to validate data in code or to include it as part of a code generation pipeline.\n\n> A key thing that sets CUE apart from its peer languages is that it merges types and values into a single concept. Whereas in most languages types and values are strictly distinct, CUE orders them in a single hierarchy (a lattice, to be precise). This is a very powerful concept that allows CUE to do many fancy things. It also simplifies matters. For instance, there is no need for generics and enums, sum types and null coalescing are all the same thing\n\n> CUE’s design ensures that combining CUE values in any order always gives the same result (it is associative, commutative and idempotent). This makes CUE particularly well-suited for cases where CUE constraints are combined from different sources:\n\n> - Data validation: different departments or groups can each define their own constraints to apply to the same set of data.\n\n> - Code extraction and generation: extract CUE definitions from multiple sources (Go code, Protobuf), combine them into a single definition, and use that to generate definitions in another format (e.g. OpenAPI).\n\n> - Configuration: values can be combined from different sources without one having to import the other.\n\n> The ordering of values also allows set containment analysis of entire configurations. Where most validation systems are limited to checking whether a concrete value matches a schema, CUE can validate whether any instance of one schema is also an instance of another (is it backwards compatible?), or compute a new schema that represents all instances that match two other schema.\n\n","n":0.057}}},{"i":15,"$":{"0":{"v":"Criteria Based","n":0.707},"1":{"v":"Need to flesh this out more, but a placeholder for my PoC: https://github.com/dustinbarnes/chaos-configuration\n","n":0.277}}},{"i":16,"$":{"0":{"v":"Practice","n":1}}},{"i":17,"$":{"0":{"v":"Site Reliability Engineering (SRE)","n":0.5},"1":{"v":"\nSite Reliability Engineers enable organisations to sustainably achieve the appropriate level of reliability in their platforms.\n\nIt applies software engineering principles to \n\nFounding principles:\n\n- Operations is a software problem.\n- Manage by Service Level Objectives.\n- Work to minimise toil.\n- Automate this year's job away.\n- Move fast by reducing the cost of failure.\n- Share ownership with developers.\n- Use the same tooling, regardless of function or job title (but APIs will outlive tools).\n\nPrimary responsibilities:\n\n- Monitor everything\n- Reduce toil through automation and problem reduction\n- Manage risk through SLIs, SLOs and an error budget\n- Documenting and sharing knowledge, encouraging best practice\n- Building resilient-enough services, early in the design phase\n- Remediating escalations\n- Carrying a pager and being on-call\n- Learn from outages using meaningful postmortems\n\n","n":0.092}}},{"i":18,"$":{"0":{"v":"Service-Level Objectives","n":0.707},"1":{"v":"Service Level Objectives specify a target level of reliability for a service by applying SLIs.\n\nSLOs provide a means of evaluating user happiness with a service's performance. Objectives should be flexible: high opportunity costs should drive loosening of an SLO:\n\n- Above the threshold, almost all users are happy with the service.\n- Below the threshold, users are likely to complain or stop using the service.\n\nRelationships to SLAs\n\nSLAs are contractual obligations, and usually incorporate a set of penalities for failure to comply. These should be based upon, but more pessimistic than, your SLOs.\n\nDefining happiness\n\nUser happiness:\n\n- drives service use;\n- drives revenue growth;\n- reduces strain on customer service; and\n- generates recommendations.\n\nDon't aim for 100%\n\n100% availability is an unrealistic target:\n\n- There's a non-zero chance that critical infrastructure components will fail simultaneously.\n- Your customers won't achieve beyond around 99.99% availability.\n- You can never ship improvements to the service: the number one source of outages is change.\n- An SLO of 100% forces you to only ever behave in a reactive fashion.\n\nExpression\n\nSLOs are expressed in the form:\n\nSLI < target\nlower bound <= SLI <= upper bound\n\nWhere curves matter, specify multiple SLOs:\n\n    90% of all RPC calls will complete in 1ms.\n    99% of all RPC calls will complete in 10ms.\n    99.9% of all RPC calls will complete in 100ms.\n\nChoosing SLOs isn't a purely technical exercise, though SREs can add value to the conversation. Work for both the product development and SRE teams should be based on them. Rules of thumb:\n\n    Past success isn't an indicator of future success. Don't base targets on current performance; things can worsen as load increases in ways which are not yet clear.\n    Keep it simple; we need to be able to reason about changes to these indicators.\n    Avoid absolutes (always available, infinitely scalable).\n    Focus: have as few SLOs as possible: if you can't prioritise work based on an SLO it isn't valuable.\n    Perfection can wait, and there's value in having these values now. Start loose and refine over time.\n    Have a safety margin. Keeping a tighter SLO internally than the one advertised externally allows room for change without disappointing users.\n    Don't over-achieve: it may be necessary to cause synthetic outages in highly available services to ensure users don't come to depend on them exceeding their SLOs.\n\nService Level Agreements\n\nSLOs should form the basis of informed Service Level Agreements with our customers. Our SLOs should be more optimistic than SLAs agreed with customers.\nSetting SLOs\n\nRemember the well-known compliance statement -- allow room for performance to slip, and tighten the SLO later:\n\n    Past performance is not an indicator of future reliability.\n\n    Choose an SLI specification.\n    Define an SLI implementation.\n    Identify coverage gaps, based on user journeys\n        Do the SLIs adequately cover user journeys and their failure modes?\n        Are there exceptions or edge cases to cover?\n        Do the SLIs capture multiple journeys with differing needs?\n    Define aspirational SLOs based on business needs.\n\nSome failure modes should be considered out of scope, especially where they're covered by existing testing methods (integration testing, unit testing, etc.).\n\nrisk = probability * impact\nrisk = time to failure * service downtime\n\nPut another way, risk can be measured in terms of three factors:\n\n    Time to detection.\n    Time to resolution.\n    Percentage of impacted users.\n\nDocument SLOs both in your monitoring system and documentation. Include:\n\n    Why the threshold is where it is.\n    Why the SLIs are appropriate.\n    Monitoring data deliberately excluded from the SLI.\n\nSLOs require iterative refinement to get right. Consider labelling them as:\n\n    In development first, while narrowing down indicators.\n    In testing when they're considered likely correct.\n    Actively paging when you're confident that false positives have been eliminated.\n\nRecognise that they're a moving target by specifying versioning from the start. Consider storing this metadata alongside the configuration for the monitoring tool, e.g. in an SCM repository.","n":0.04}}},{"i":19,"$":{"0":{"v":"Service-Level Indicators","n":0.707},"1":{"v":"\nService Level Indicators, or SLIs, are ***quantitative*** measures of an aspect of the delivered service.\n\n## SLI types\n\nBase SLOs on metrics that matter to the particular workload:\n\n- Request/response workloads involve clients making requests and awaiting responses.\n    - Availability -- either in terms of time or request yield, are we online and reachable?\n    - Latency -- how long does it take to return a response?\n    - Quality -- what percentage of traffic is gracefully degrading due to overload or partial outages?\n- Data processing take records as input, mutate them and emit them somewhere else.\n    - Freshness -- how recent the data of the output is in relation to its inputs.\n    - Correctness -- the proportion of data generating correct output.\n    - Coverage -- the proportion of valid data processed successfully.\n    - Throughput -- the proportion of time where the processing rate is faster than a threshold.\n- Storage systems accept data for later retrieval.\n    - Durability -- the proportion of written records which can be successfully read.\n    - Throughput -- the proportion of records that can be written or read within a given time unit.\n    - Latency -- the time taken for a written record to be read.\n\nBe judicious in selection: ensure you're targeting things that users value, as there's a human cost to each metric monitored. Systems generally fit into a classification:\n\n- User-facing systems, e.g. search engines, care about availability, latency and throughput.\n- Storage systems emphasise latency, availability and durability.\n- Big data systems care about throughput and end-to-end latency.\n- All systems care about correctness, though this is outside of the SRE purview and usually lies with the product team.\n\nCollection is usually server-side, either using time series data or periodic log analysis. Don't forget the client-side though: richer applications are prone to slowdowns and bugs that won't be detectable from the server-side, and these still hurt user experience.\n\nUse distributions over averages for aggregation to catch long tail. Percentiles (50th, 99th and 99.9th, for instance) will show the average case whilst still allowing us to identify pathological cases.\n\nStandardising indicators allows shared understanding without having to always reason about the meaning of a metric:\n\n- Intervals (averaged over 1 minute)\n- Regions (all tasks in a cluster)\n- Measurement frequency (every 10 seconds)\n- Which transactions are included (HTTP PUTs from tool X)\n- Acquisition (server-side, client-side)\n- Data-access latency (time to first byte, time to last byte)\n\nMeasurement strategies\n\n- Application-level metrics\n- Logs processing\n- Frontend infrastructure metrics\n- Synthetic clients/data\n- Client-side instrumentation\n\nVerification\n\n- Can we measure the SLIs we've created where we want to measure them, given this infrastructure?\n- Do the SLIs adequately capture the user journey and its failure modes?\n- Are there any exceptions or edge cases to consider?\n- Do the SLIs capture multiple journeys with differing requirements?\n","n":0.047}}},{"i":20,"$":{"0":{"v":"SRE Seeks Simplicity","n":0.577},"1":{"v":"Simplicity is efficiency.\n\nJohn Gall:\n\n> A complex system that works is invariably found to have evolved from a simple system that worked. A complex system, designed from scratch never works and cannot be patched to make it work. You have to start over, beginning with a simple working system.\n\nMeasures of complexity\n\n- Cylomatic complexity measures the number of paths (in the form of branches and loops) through software source code.\n- Training time for new hires.\n- How much explanation time is required for a comprehensive, high-level overview.\n- How much administrative diversity is enabled through configuration.\n- The diversity between deployed environments.\n- Age of the system, per Hyrum's Law.\n\nRules of thumb\n\n- Deal with complexity upfront (e.g. API design, typing and documentation for forward and backward compatibility).\n- Consider the full lifecycle of rewrite projects, including developing against a moving target, a full migration plan and additional costs incurred during migration.\n","n":0.083}}},{"i":21,"$":{"0":{"v":"SRE - On Call","n":0.5},"1":{"v":"The on-call effort is about minimising detection and resolution time. The postmortem process should raise and follow up on action items identified as causes of incidents, reducing time to next failure.\n\nPut in place tools\n\n- Playbooks define a linear set of actions to take in response to types of incident. Aim for one entry per alert.\n- Runbooks extend playbooks by containing conditional steps. They may be executed automatically, optionally relying on external human decision making.\n\nCreate training roadmaps\n\nIt's important to prepare engineers for their on-call shifts, so define an onboarding process. For example:\n\n- Administering production jobs\n- Interpreting debug data\n- Draining traffic from a cluster\n- Rolling back a bad release\n- Blocking/rate-limiting unwanted traffic\n- Adding capacity: scaling up/out\n- Using monitoring systems (alerting, dashboards, logs)\n- Describing architecture, key components and dependencies\n\n- Aim to build muscle memory. Running regular \"wheel of misfortune\" sessions may help teams gain understanding of common failure scenarios.\n\nReview guidelines for process:\n\n- Reading handoff from the previous shift at shift start.\n- Minimise impact first, then address underlying cause.\n- Escalate when...\n- Send handoff to the next shift at shift end.\n\nReview pager load\n\nAlerts should be immediately actionable. Thoroughly test new pages before making them live.\n\nInputs\n\n- Production:\n    - Existing bugs\n    - Newly introduced bugs\n    - Speed of new bug identification\n    - Speed of bug mitigation and removal\n- Alerting:\n    - Alerting thresholds for paging alerts\n    - Introduction of new paging alerts\n    - SLO alignment with upstream services' SLOs\n- Human processes:\n    - Rigour of fixes and bug follow-up\n    - Quality of data collected about paging alerts\n    - Attention paid to pager load trends\n    - Human-initiated production changes\n    - Quality of data collected on operational issues, between shifts in handoff\n    - Vigilance given to operational overload\n\nResolutions\n\n- Address bugs as a priority: consider rolling back upon investigation of a bug, fixing it and then rolling forward rather than leaving known-bad releases in production. This reduces impact to the error budget. Roll out features behind flags so that they can easily be disabled when problems arise.\n- Consider automating changes currently made by humans, allowing for greater validation and testing.\n- Reduce identification delay by changes to alerting and the operational playbook. Practice emergency response, limit the size of releases and log changes. Know when to ask for help.\n\nScheduling\n\nOn-call shifts should be kept short to minimise impact on mental health. Scheduling should take into account personal circumstances in order to be fair, accommodating different social commitments and allowing for swapping of shifts in the short-term. Document the process. Tools like PagerDuty may help.\n\nTime spent on-call should be compensated, either through time off in lieu or monetary reward. Pay attention to local labour laws.","n":0.048}}},{"i":22,"$":{"0":{"v":"15-Factor App","n":0.707},"1":{"v":"\nThis is an extension of the [[tech.practice.12-factor-app]], and is an update given the takeover of container technology, K8s in particular. \n\n## 1. One Codebase\n\n- A version control system like Git always tracks the fifteen-factor app.\n- If we have multiple codebases, it’s not an app but a distributed system.\n- If multiple apps share the same code, it violates the fifteen-factor. The solution is to add the shared code as a dependency.\n- There is only one codebase per app, but there are many deploys of the app. The app is the same across all deploys, but different versions may be active in each deploy.\n\n## 2. Dependencies\n\n- A fifteen-factor app declares all dependencies, completely and exactly, via a dependency declaration manifest. It simplifies the setup for developers new to the app. The new developers will set up everything by running the app’s code with a deterministic build command.\n- A fifteen-factor app doesn’t rely on the implicit existence of any system tools. If the app needs to shell out to a system tool, that tool should be vendored into the app.\n\n## 3. Config\n\n- Never store constants in the code. To be fifteen-factor compliant requires a strict separation of config from code.\n- Config varies across deploys; code doesn’t.\n- Always prefer storing the config in environment variables; environment variables are easy to change between deploys without changing any code.\n- Environment variables are fully orthogonal to other environment variables; they are never grouped but independently managed for each deployment.\n\n## 4. Backing Services\n\n- Backing Service: any service the app consumes over the network as part of its normal operation.\n- Systems administrators who deploy the app’s runtime manage the backing services.\n- There are local and third-party services, and a fifteen-factor app doesn’t make any distinction between them.\n- Deploy a fifteen-factor app should swap out a local backing service with one managed by a third party without any changes to the app’s code.\n- Each particular backing service is a resource, called attached-resource.\n\n## 5. Build, release, run\n\n- The build stage is a process that converts a code repo into an executable bundle known as a build.\n- The release stage takes the build and combines it with the deploy’s current config.\n- The run stage runs the app in the execution environment.\n- Each stage is strongly separated.\n\n## 6. Processes\n\n- Fifteen-factor processes are stateless and share-nothing. We have to use a stateful backing service whether we want to store any data.\n- We can use the memory space or filesystem as a temporary, single-transaction cache.\n- Never use sticky sessions. Never assume that anything cached in memory or disk will be available on a future request or job.\n\n## 7. Port binding\n\n- The fifteen-factor app is completely self-contained.\n- The web application exports HTTP as a service by binding to a specific port and listening to requests coming in on that port.\n- Note also that the port-binding approach means that one app can become the backing service for another app.\n\n## 8. Concurrency\n\n- Processes are first-class citizens.\n- We should design our application to distribute workload across multiple processes. Individual processes can leverage a concurrency model like Thread internally.\n- Fifteen-factor app processes should never daemonize or write PID files, instead, rely on the operating system’s process manager.\n\n## 9. Disposability\n\n- The application should be disposable; it means that we can start or stop it at any moment.\n- We should aim to minimize startup time. The process should take a few seconds from the time the launch command is executed until the process is up and ready to receive requests or jobs.\n- Processes should also be robust against sudden death. A fifteen-factor app is designed to handle unexpected terminations.\n\n## 10. Dev/prod parity\n\n- The fifteen-factor app is designed for continuous deployment by keeping the gap between development and production small.\n- The time between deploys should be just hours than weeks.\n- The author of the code and who deploy the application is the same person.\n- Dev env and prod env should be as similar as possible.\n\n## 11. Logs\n\n- Logs are the stream of aggregated, time-ordered events collected from the output streams of all running processes and backing services.\n- Fifteen-factor app never concerns itself with routing or storage of its output stream. The application should write its event stream, unbuffered to the standard output.\n- The stream can be sent to a log indexing and analysis system like Splunk to find past events, graph trends, and active alerting.\n\n## 12. Admin processes\n\n- One-off admin processes should be run in an identical environment as the regular long-running processes of the app.\n- Admin code must ship with application code to avoid synchronization issues.\n\n## 13. API First\n\n- We should define the service contract first to help our consumers understand what to send and receive, defining a common contract.\n- This approach enables consumers and service developers to work in parallel.\n- It helps avoid bottlenecks and facilitates the virtualization of the APIs so the consumers can run the tests against the mocks.\n\n## 14. Telemetry\n\n- Monitoring our microservices, containers, and env help us to scale, self-heal and manage alerts for end-users and platform operators.\n- We can use machine learning towards those metrics to derive future business strategies.\n- We can observe the application’s performance, stream of events and data, health checks, when the application starts, scales, and so on.\n\n## 15. Authentication\n\n- Make sure all security policies are in place. Developers tend to underestimate the importance that security has.\n- APIs should be secured using OAuth, RBAC, etc.\n- Web content should be exposed externally on HTTPS\n- MFA\n- Database protection\n","n":0.033}}},{"i":23,"$":{"0":{"v":"12-Factor App","n":0.707},"1":{"v":"\nIn the modern era, software is commonly delivered as a service: called web apps, or software-as-a-service. The twelve-factor app is a methodology for building software-as-a-service apps that:\n\n> - Use declarative formats for setup automation, to minimize time and cost for new developers joining the project;  \n> - Have a clean contract with the underlying operating system, offering maximum portability between execution environments;  \n> - Are suitable for deployment on modern cloud platforms, obviating the need for servers and systems administration;  \n> - Minimize divergence between development and production, enabling continuous deployment for maximum agility;  \n> - And can scale up without significant changes to tooling, architecture, or development practices.\n\nThe twelve-factor methodology can be applied to apps written in any programming language, and which use any combination of backing services (database, queue, memory cache, etc).\n\n# The 12 Factors\n1. [[tech.practice.12-factor-app.codebase]]  \nOne codebase tracked in revision control, many deploys\n2. [[tech.practice.12-factor-app.dependencies]]  \nExplicitly declare and isolate dependencies\n3. [[tech.practice.12-factor-app.configuration]]  \nStore config in the environment\n4. [[tech.practice.12-factor-app.backing-services]]  \nTreat backing services as attached resources\n5. [[tech.practice.12-factor-app.build-release-run]]  \nStrictly separate build and run stages\n6. [[tech.practice.12-factor-app.processes]]  \nExecute the app as one or more stateless processes\n7. [[tech.practice.12-factor-app.port-binding]]  \nExport services via port binding\n8. [[tech.practice.12-factor-app.concurrency]]  \nScale out via the process model\n9. [[tech.practice.12-factor-app.disposability]]  \nMaximize robustness with fast startup and graceful shutdown\n10. [[tech.practice.12-factor-app.dev-prod-parity]]  \nKeep development, staging, and production as similar as possible\n11. [[tech.practice.12-factor-app.logs]]  \nTreat logs as event streams\n12. [[tech.practice.12-factor-app.admin-processes]]    \nRun admin/management tasks as one-off processes\n","n":0.066}}},{"i":24,"$":{"0":{"v":"Processes","n":1},"1":{"v":"> Execute the app as one or more stateless processes\n\n\n- Apps are executed in the execution environment as one or more processes.\n- Processes are stateless and share nothing.\n    - Persistence takes place in stateful [[tech.practice.12-factor-app.backing-services]].\n- Execution environment memory and filesystem resources are brief, single-transaction caches.\n    - Never assume anything is cached in memory and available for future transactions.\n    - Relocation will mean starting from a cold cache.\n    \n\nNo sticky sessions!\n","n":0.12}}},{"i":25,"$":{"0":{"v":"Port Binding","n":0.707},"1":{"v":"> Export services via port binding.\n\n\n- Web apps are sometimes executed inside webserver containers.\n- Services export their native protocol by binding a port and listening for traffic on it.\n    - API Gateway\n- This enables one app to be a [[tech.practice.12-factor-app.backing-services]]  for another by referencing it in the consuming app's [[tech.practice.12-factor-app.configuration]].\n\n","n":0.141}}},{"i":26,"$":{"0":{"v":"Logs","n":1},"1":{"v":"> Treat logs as event streams\n\n- Logs provide insight into a running app's behaviour.\n- Logs are streams of aggregated, time-ordered events collected from output streams of [[tech.practice.12-factor-app.processes]] and [[tech.practice.12-factor-app.backing-services]].\n    - Raw form is typically text.\n    - We can mutate them into more structured data formats.\n    - No fixed beginning or end.\n- Apps don't concern themselves with the routing or storage of their output.\n    - They write to stdout.\n    - Local execution during development allows developers to see the logs via their terminal.\n    - In deployed environments we can capture and retain this stream.\n        \n- The event stream is handled by the deployment environment, enabling:\n    - Finding historical events.\n    - Graphing trends.\n    - Active alerting.\n","n":0.094}}},{"i":27,"$":{"0":{"v":"Disposability","n":1},"1":{"v":"> Maximize robustness with fast startup and graceful shutdown\n\n- [[tech.practice.12-factor-app.processes]] are dispoable.\n    - Elastic scaling\n    - Rapid deployment of code or [[tech.practice.12-factor-app.configuration]]\n- Minimise startup time.\n    - Ready in second(s) from cold\n    - Easier to relocate\n    - Easier to use spot instances\n- [[tech.practice.12-factor-app.processes]] shut down gracefully\n    - Stop listening on the service [[port|tech.practice.12-factor-app.port-binding]]\n    - Let current transactions complete (draining)\n- [[tech.practice.12-factor-app.processes]] should be robust against sudden death in case of hardware failure:\n    - Queueing backend\n    - Return jobs to the queue on failure.\n\n\n","n":0.111}}},{"i":28,"$":{"0":{"v":"Dev Prod Parity","n":0.577},"1":{"v":"> Keep development, staging, and production as similar as possible.\n\n- Gaps between development and production manifest in three areas:\n    - **The time gap:** A developer may work on code that takes days, weeks, or even months to go into production.\n    - **The personnel gap:** Developers write code, ops engineers deploy it.\n    - **The tools gap:** Developers may be using a stack like Nginx, SQLite, and OS X, while the production deploy uses Apache, MySQL, and Linux.\n\n- Apps are designed for Continuous Delivery by shrinking these gaps.\n    - Make the time gap small: a developer may write code and have it deployed hours or even just minutes later.\n    - Make the personnel gap small: developers who wrote code are closely involved in deploying it and watching its behavior in production.\n    - Make the tools gap small: keep development and production as similar as possible.\n\nGaps === risk\n","n":0.083}}},{"i":29,"$":{"0":{"v":"Dependencies","n":1},"1":{"v":"> Explicitly declare and isolate dependencies.\n\n* Never relies on implicit existence of system-wide packages. \n    * It declares all dependencies, completely and exactly, via a dependency declaration manifest.\n\n* Basically, use a dependency management tool. \n  * Java: Maven, Gradle\n  * Node: npm, yarn\n  * etc","n":0.149}}},{"i":30,"$":{"0":{"v":"Configuration","n":1},"1":{"v":"> Store configuration in the environment.\n\n\n- Configuration is anything that is likely to very between deploys\n- Strict separation of configuration from code; no configuration stored in constants.\n  - Good litmus test: could we open source this today without compromising credentials?\n- Use of files not checked into version control alongside code is best practice.\n  - .env/dotenv\n","n":0.135}}},{"i":31,"$":{"0":{"v":"Concurrency","n":1},"1":{"v":"> Scale out via the process model\n\n- Historically we scaled via OS processes\n    - Large virtual machines like JVM favour allocating one large uberprocess and facilitating concurrency via threads.\n- [[tech.practice.12-factor-app.processes]] are a first-class citizen.\n    - HTTP requests via a web process, long-running background tasks via worker processes.\n- Individual [[tech.practice.12-factor-app.processes]] can do their own multiplexing using threads/green threads.\n    - Share-nothing, horizontally partitionable process model simplifies scaling out.\n- Never daemonise or write PID files.\n- Rely on the OS process manager (e.g. systemd), cloud platform, etc.\n    - The process manager should take care of output streams, crashed processes and operator-initiated events like restarts/terminations.\n","n":0.1}}},{"i":32,"$":{"0":{"v":"Codebase","n":1},"1":{"v":"> One codebase tracked in revision control, many deploys\n\n\n- Everything always tracked in version control.\n- 1:1 correlation between the codebase and the app\n  - Mutiple codebases represent a distributed system. Each component can individually comply with the principles.\n  - One codebase per app, but many deploys (running instances).","n":0.144}}},{"i":33,"$":{"0":{"v":"Build Release Run","n":0.577},"1":{"v":"> Strictly separate build and run stages\n\n- Three stages:\n    - **Build** is a transform of source code into an executable bundle called a build. Using a specified commit, this stage fetches dependencies and compiles binaries/assets.\n    - **Release** takes the build produced above and combines it with a deploy's Configuration. The release contains both the build and the config, ready for immediate execution.\n    - **Run** runs the app in an execution environment by launching processes against a selected release.\n  \n  - Releases should be uniquely identifiable from a timestamp or version identifier.\n    - Releases are an append-only ledger and a release cannot be mutated once it is created. Any change must create a new release.\n- Shift left: the run stage should be kept as simple as possible to ensure engineers aren't required for recovery during unsociable hours.\n","n":0.086}}},{"i":34,"$":{"0":{"v":"Backing Services","n":0.707},"1":{"v":"> Treat backing services as attached resources\n\n- Backing service is defined as any service the app consumes over the network in normal operation.\n    - Datastores (databases, blob storage)\n    - Messaging/queueing\n    - Mail (SMTP, IMAP)\n    - Caching\n- No distinction between local and third-party services.\n    - They're both just attached resources, accessed via a URL or some other locator using credentials stored in Configuration.\n- Misbehaving resources can be replaced without changes to code.\n\n","n":0.119}}},{"i":35,"$":{"0":{"v":"Admin Processes","n":0.707},"1":{"v":"> Run admin/management tasks as one-off processes.\n\n\n- One-off maintenance:\n    - Database migrations.\n    - Console to inspect its state or mutate data.\n    - Running one-time maintenance scripts.\n- Run in identical environment\n    - Against a release, using the same Codebase and Configuration.\n    - Admin code must ship with application code.\n","n":0.144}}},{"i":36,"$":{"0":{"v":"Philosophy","n":1},"1":{"v":"This is a collection of things that may not be quantifiable, and are often subjective. Be judicious in applying these things, as too much rigidity is far worse. ","n":0.189}}},{"i":37,"$":{"0":{"v":"Versioning","n":1},"1":{"v":"\n# What is the Purpose a Software Version?\nThe only actual, functional requirement of a version is that it uniquely identifies a build of software. \n\nThat's all a version does. Everything else is folks applying their comfort levels and background to how it should be done. \n\n## What a Version Does Not Necessarily Do\n\n### Tell which version is newer\n\nThis is not a requirement of a version. It's simply a practice that most people have adopted out of convenience and convention. When you're trying to figure out what version scheme to adopt, you'll likely make this a point of consideration. \n\nYou might choose to drop this convention in CI/CD pipelines, as the git commit ref can often suffice as the version. \n\n## Tuple of dot-joined numbers\n\nThis is a convention of the industry, but not required. Some tools version based on codenames, and most of the industry has accepted some postfix string to further classify builds (-alpha, -beta, rc1, etc)\n\nThen, you have to consider if you're zero-padding, how many items to have (3 is conventional, 2 and 4 are not uncommon). \n\n\n# Some of the options\n1. [[tech.philosophy.versioning.semver]]\n1. CalVer\n1. ZeroVer/0ver\n1. SentimentalVer\n1. SemVer-ish\n\n# Version Differences Between Libraries and Services\n- services should generally use calver\n- libraries should use SemVer-ish","n":0.07}}},{"i":38,"$":{"0":{"v":"SemVer","n":1},"1":{"v":"\n# SemVer: Pros and Cons\n\nSemVer is a great theoretical idea. However, there are some issues\n\n## SemVer: In Theory\n\nSemVer consists of 3 dot-separated numbers:\n\n![](/assets/images/2023-03-24-17-31-01.png)\n\nToolchains within NodeJS, Ruby, and Python claim adherence to this. When you import a dependency, it will often show up like this: \n\n```json\n\"node-tool-lib\": \"^2.1.4\"\n```\n\nWith this specification, you can pick up a version of `node-tool-lib` from `2.1.4` to `< 3.*.*`. So long as the authors maintain the SemVer conventions, it should be safe to upgrade at any time. \n\n## The Contract Problem\n\nSemVer is designed to be used as an indicator of compatibility and functionality; the numbers have rigid meanings. The argument goes that a major release is the only time backwards-incompatible changes are made, thus indicating safety in upgrading the minor- and patch-level as available. But this means that multiple things need to be true at the same time:\n\n1. You have a well-defined contract, and a well-defined place that contract is applied\n2. You are able to enforce the contract through programatic means, such as testing\n3. You absolutely must bump the major version on an incompatibility, no matter how slight\n4. The major version number is only incremented programmatically based upon your contract\n\nNow, almost nobody has \\(1\\) and \\(2\\) -- and no, JSON Schema validation is not sufficient for this purpose. Contracts are not just the shape of the data, but also the required order of operations as well as the exceptional code paths. Item \\(3\\) is a critical feature we'll discuss in a moment, but item \\(4\\) is how you maintain compatible with SemVer. No human developer is perfect. If we rely on humans to decide what constitutes a breaking change, we will invariably end up in a situtation where a developer thought they fixed a small bug, only bumped the patch, but actually introduced a new bug that wasn't caught by the automated testing framework.\n\n## The CI/CD Problem\n\nUsing SemVer-based ranges for dependencies means that your build process is not strictly repeatable. It depends upon the availability of updates to your dependencies, thus dependent upon time of day that the build is executed. Old code that previously worked with thorough testing may no longer work if built a few days later. This happens because some developers aren't as strict with their SemVer as they should be. Other developers decided they didn't like SemVer, and refused to be bound by it. [You can guess how that went](https://github.com/jashkenas/underscore/issues/1805). Lockfiles are a poor band-aid for a \"design feature\". One thing I insist on in my projects is that every version is exactly pinned. No SemVer ranges, no \\`-SNAPSHOT\\` dependencies, etc. In NPM, one can enforce this with the \\`--save-exact\\` flag when running \\`npm install\\`.\n\n### Zero-Dot\n\nSo what is a `0.y.z` release? SemVer tells us that: \n\n> Major version zero \\(0.y.z\\) is for initial development. Anything may change at any time. The public API should not be considered stable  \n>  - [https://semver.org/\\#spec-item-4](https://semver.org/#spec-item-4)\n\nThus, if a dev depends on package 0.y.z in a SemVer range, you should plan on receiving breaking changes. That's not very helpful. How many projects sit out there at 0.y.z forever? \n\nIn a strictly pedantic sense, you can't claim to do SemVer until you have a 1.y.z release. 0.y.z releases are just a beginning state, not a public availability state. \n\n## What does Compatibility Mean\n\nNotice in most large nodejs repositories, authors pin their dependencies to exact versions. This is to get away from the problems that SemVer introduced. \n\nPop quiz: How are devs bumping the major version number? Remember what SemVer says:\n\n> Major version X \\(X.y.z \\| X &gt; 0\\) MUST be incremented if any backwards incompatible changes are introduced to the public API.  \n> - [https://semver.org/\\#spec-item-8](https://semver.org/#spec-item-8)\n\nTo answer this effectively, we must strictly define what constitutes compatibility. Earlier in the document, this is what they say about the patch-level version:\n\n> Patch version Z \\(x.y.Z \\| x &gt; 0\\) MUST be incremented if only backwards compatible bug fixes are introduced. A bug fix is defined as an internal change that fixes incorrect behavior.  \n> - [https://semver.org/\\#spec-item-6](https://semver.org/#spec-item-6)\n\nThis sentiment is intuitive and obvious. However, remember that we are saying that those who depend on us may automatically upgrade safely in the patch-level. \n\nSemVer declares the public API as what you're versioning. Let's imagine, however, that there's a change to your 500 error response. Instead of just sending a JSON document like below:\n\n`{ \"error\": \"500\", \"cause\": \"Some error occurred\"}`\n\nThen, you want to enhance the output to also include a tracing id that can be used for later analysis:\n\n`{ \"error\": \"500\", \"cause\": \"Some error occurred\", \"trace\": \"13eb8a9\"}`\n\nThis, by definition, breaks the output contract, right? Or, did your contract specify that clients should accept all new fields? The default behavior on extra fields changes between libraries. Some throw exceptions, others silently swallow the extra data. Thus, what \"backwards compatible\" means is specific to the implementers of your API. \n\nArguably, any change in outputs from the universe of inputs is a public-facing, non-backwards-compatible change. Thus simply adding this field to your outputs constitutes a major version bump. Without tools specifically designed to enforce the contracts, you may never know if a major bump was needed.\n\nThis therefore usually leaves to humans who decide that a bump is needed, based on some abstract concept of compatibility. Without a strict compatibility test, this process will fail, given enough time.\n\n## Your Major Version Sucks\n\nSo you're all-in on SemVer. You have your contracts, you have your contract tests, you have a CI system that can divine the major/minor/patch-level changes. Now, you're faced with a new problem: You will invariably be bumping your major version far more than most folks would like. You should be expecting a major version with 3 or 4 digits. NVidia's graphics drivers are relatively famously large-versioned \\(but strictly not SemVer\\). At the time of writing, the latest driver was version \\`417.21\\`. Without a patch level, however, this is not SemVer.\n\nThis leads to two schools of thought. Some folks claim that the major version is more of a marketing/effort construct, and that you can do SemVer with just the last two digits. Again: this isn't SemVer. SemVer's primary rule, in my opinion, is that the major version number indicates compatibility.\n\nOthers think that it's fine to continually bump the major version number, and there's no big issue with having major version numbers in the hundreds \\(\\`339.0.1\\`\\) or thousands \\(\\`1317.3.19\\`\\). I argue that this is actually the only true version of SemVer. However, this assumes that you have the contracts in place and verified. Not only is it not enough for a developer to make a judgement call, having a human bump the major version is not strictly SemVer.\n\nThe sad reality is, most who claim to be doing SemVer aren't doing SemVer. They don't actually have their compatibility contracs being enforced, much less even existing. Without a non-human arbiter of truth, this SemVer-lite is just another iteration of manual versioning, but with a lot more risk for downstream dependencies due to the implied safely\n\n## Compatibility Effort\n\nSo given my assertion that the only folks who truly do SemVer will necessarily have major version numbers that grow much larger than most are used to, what is the benefit of SemVer? Yes, you can say you want \\`113.x.x\\` and probably get the right thing. However, how do we know how much effort it will take for downstream dependencies to update major versions? In a project with traditional versioning, I know that going from version 2 to version 3 will mean big changes, possibly some code fixes, possibly complete re-workings of parts of my codebase. With SemVer, this level of change is at the same level as a small bug fix or inconsequential data shape change.\n\nDownstream users then have to go through and look at all the major version bumps, and actually read the release notes, for potentially hundreds or thousands of major versions, and determine what adaptations are needed in their code.\n\n## The CI/CD Alternative\n\nWhile some developers have taken to what has been humorously called \\[Sentimental Versioning\\]\\(http://sentimentalversioning.org/\\). Basically, developers still follow the old way of the major version being a marketing construct, then try to enforce compatibility on the feature- and patch-level version. This seems inferior, but it also gives downstream users a conceptual idea of how big the change is. When Angular went from Angular 2 to Angular 4, it was a clear message to the community that \\*this new thing is really different\\*. There's an explicit declaration of integration efforts. I can be relatively certain that \\`2.0.1\\` and \\`2.0.10\\` will \\*probably\\* be interchangeable. But it's not guaranteed through an automated, robotic means, and thus building your product around auto-upgrading SemVer ranges introduces surface area for tricky regressions.\n\nMy preferred versioning scheme is specific to CD pipelines, but as a proponent of CI/CD, I think you should adopt it anyway! So in this world, we don't care about major-minor-patch. We're just delivering iterations of the same product. We have an integration environment for the specific purpose of testing with dependencies. In this case, all that really matters is that you have a unique version for your automated deployments to use. In my preference, the version is exactly the build number in the CI system of your choice. This has some nice side effects: You can immediately tie a version back to the build that created it; The number is monotonically increasing, indicating directionality of progress. Another potential is to simply use the git hash of the commit as your version. While this can work with tools, it doesn't really indicate directionality of progress nor does it indicate which build is newer without additional metadata.","n":0.025}}},{"i":39,"$":{"0":{"v":"SemVer Requirements","n":0.707},"1":{"v":"# Requirements\nThere are a few things that must be done if a project or team plans to adopt SemVer\n\n## Requirement 1: Public API\nSemVer is very clear that you must declare a public API. However, the SemVer spec itself is relatively vague on what this means, in item 1 of the specification:\n\n> Software using Semantic Versioning MUST declare a public API. This API could be declared in the code itself or exist strictly in documentation. However it is done, it SHOULD be precise and comprehensive.\n\nNote the purposeful use of MUST and SHOULD in these cases. You MUST declare a public API, but it only SHOULD be precise and comprehensive. Relaxing this requirement breaks all of SemVer, in this author's opinion. \n\nThen, item 5 in the spec comes along: \n\n> Version 1.0.0 defines the public API. The way in which the version number is incremented after this release is dependent on this public API and how it changes.\n\nIf we have an imprecise and non-comprehensive documentation suite, there is still a chasm of variability in what would be considered breaking or not. \n\n## Requirement 2: Not 0-version\nThe SemVer spec in item 5 is clear that 1.0.0 is where you determine how to bump the version for future changes. So long as you keep the `MAJOR` part set to `0`, you're free to do anything you want, including breaking downstream users. \n\n## Requirement 3: Bumping MAJOR\nYou MUST bump the major version when ANY breaking change is introduced to the public API. Yes, ANY breaking change. \n\n","n":0.063}}},{"i":40,"$":{"0":{"v":"Purpose of SemVer","n":0.577},"1":{"v":"\n# SemVer Purpose\nSemVer is a way to version software such that compatibility can be programatically communicated to downstream users. \n\nThis is done to allow package managers to automatically pick up compatible changes. In theory, this allows downstream users to stay up-to-date on security and bug fixes without having to think about it. Therefore, only when the `MAJOR` version changes do they have to spend effort updating. ","n":0.123}}},{"i":41,"$":{"0":{"v":"Promise of SemVer","n":0.577},"1":{"v":"\nc.f. https://semver.org\n\n> Given a version number `MAJOR.MINOR.PATCH`, increment the:\n> 1. `MAJOR` version when you make incompatible API changes\n> 2. `MINOR` version when you add functionality in a backwards compatible manner\n> 3. `PATCH` version when you make backwards compatible bug fixes\n>\n> Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.\n\nThe promise is that so long as `MAJOR` doesn't change, nothing will break and you can accept the new version without issue. The special exception is when `MAJOR` is `0`.","n":0.109}}},{"i":42,"$":{"0":{"v":"SemVer Breaking Changes","n":0.577},"1":{"v":"\n# What is a Breaking Change?\nThis seems easy, but what does \"breaking\" mean? [[tech.philosophy.laws.hyrum]] says that at some usage level, any behavior in your system is depended upon by downstream users. \n\nThis means that, if you are doing active work to your project, you'll quickly be in double- or triple-digits for your major version. Some may say this means a poor up-front design, but it ignores some real-world concerns. Let's walk through some examples:\n\n## Behavioral Change\nInitial code:\n\n```\nfunction doThing(text) {\n    return text.replace('<>', 'Your Name Here');\n}\n```\n\nNew code:\n\n```\nfunction doThing(text) {\n    return text\n      .replace('<>', 'Your Name Here')\n      .replace('[]', 'Your Address Here');\n}\n```\n\nContractually and by the public API, nothing has changed. It's still a `f(string) => string` function. However, users that depended upon '[]' passing through unfiltered will now experience a break.\n\nThis is a breaking change, and requires a major version bump -- unless you explicitly documented that things like `[]` or `()` are future reserved tokens. \n\n## Performance Changes\nIf you introduce a change where you have the same function, with the same shape, but the previous version executed in O(1) time, and the new version executes in O(N) time.\n\nWithout any changes to the API interface itself, you're introducing a potentially breaking change. \n\n## Automated Testing\nYour automated test are the closest most teams have to Consumer Contract Testing. Any time a test breaks, it indicates a behavior change. Ergo, any time you fix a test, that's a major version bump. \n\nBy changing the behavior of your contract (say, by fixing a bug) and updating your automated tests (to ensure new behavior persists, and to prevent regression), you are establishing a break to your API. \n\n## They're All Potentially Breaking!\nThere's a very strong argument to be made that every change to the system is a breaking change. I know it sounds like pedantic wordplay, but we're talking about a versioning scheme whose primary purpose is to allow the automatic and error-free update of dependency upgrades (and their transitive dependencies). \n\nSo long as a human is hedging the definition of a breaking change or staying at ZeroVer or somehow putting their fingers on the scale, this effort is insufficient. \n","n":0.053}}},{"i":43,"$":{"0":{"v":"Shapes","n":1},"1":{"v":"Oftentimes I will refer to the \"shape\" of a function or data. The idea is that the shape of these things is how you determine compatibility. I think of this as a less-than-formal specification. \n\n## Function Shapes\nFunction shapes are based on the inputs and outputs. It's basically the function signature. \n\nCommon function shapes:\n> Please note `()` being used to represent 'void'\n\n- Void: `() -> ()`\n- Unary: `T -> R`\n- Binary: `(T, U) -> R`\n- Ternary: `(T, U, V) -> R`\n- Supplier: `() -> R`\n- Consumer: `T -> ()`\n- Predicate: `T -> bool`\n- Binary Prediate: `(T, U) -> bool`\n\n## Data Shapes\n\nThe shape of a dataset refers to the expected fields, expected nested objects, etc. It's an informal way of saying the format. I usually use this terminology when consuming raw REST APIs, and not using an SDK to provide schema validation or object mapping. ","n":0.084}}},{"i":44,"$":{"0":{"v":"Software Development Laws","n":0.577},"1":{"v":"\n![[tech.philosophy.laws.*#^begin]]","n":1}}},{"i":45,"$":{"0":{"v":"Murphy's Law","n":0.707},"1":{"v":"\n> If something can go wrong, it will.\n\n- First derivation: If it works, you probably didn't write it.\n- Second derivation: Cursing is the only language all programmers speak fluently.\n- Conclusion: A computer will do what you write, not what you want.\n","n":0.156}}},{"i":46,"$":{"0":{"v":"Hyrum's Law","n":0.707},"1":{"v":"\n> All behaviors of your APIs, be them voluntarily or not, will be depended upon, and therefore create friction to change.\n\n- aka The Law of Implicit Interfaces\n\nhttps://www.hyrumslaw.com/\n\n\n> With a sufficient number of users of an API,  \n> it does not matter what you promise in the contract:  \n> all observable behaviors of your system  \n> will be depended on by somebody.\n","n":0.128}}},{"i":47,"$":{"0":{"v":"Hofstadter's Law","n":0.707},"1":{"v":"\n> It always takes longer than you expect, even when you take into account Hofstadter's Law.","n":0.25}}},{"i":48,"$":{"0":{"v":"Conway's Law","n":0.707},"1":{"v":"\n> Any piece of software reflects the organizational structure that produced it.\n\nOr even more clearly:\n\n> Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations.\n\nIn many organizations teams are divided according to their functional skills. So you'll have teams of front-end developers, backend-developers and database developers. Simply stated, it's hard for someone to change something if the thing he/she wants to change is owned by someone else.","n":0.115}}},{"i":49,"$":{"0":{"v":"Brook's Law","n":0.707},"1":{"v":"\n> Adding manpower to a late software project makes it later.\n\nRef. book Mythical Man Month\n","n":0.258}}},{"i":50,"$":{"0":{"v":"80/20 Rule","n":0.707},"1":{"v":"> For many phenomena, 80% of consequences stem from 20% of the causes.\n\naka Pareto Principle \n\n","n":0.25}}},{"i":51,"$":{"0":{"v":"The Business Value of Automated Testing","n":0.408},"1":{"v":"\nThis article has been moved to [[articles.business-value-automated-testing]]. \n\nThis url will be removed on or after July 1, 2023.","n":0.236}}},{"i":52,"$":{"0":{"v":"Concepts","n":1},"1":{"v":"This is a non-exhaustive grouping of concepts around building and running software. As I interlink more things, it becomes clear that certain concepts needed a page. The pages under this grouping will likely benefit you most if you read the backlinks section at the bottom of the document. ","n":0.144}}},{"i":53,"$":{"0":{"v":"Developer Interviews","n":0.707},"1":{"v":"Developer interviews are a tough thing to do well. You aren't trying to out-smart the candidate, and you don't \"win\" if you create questions that nobody solves. You need to balance the filter such that you bring on people who you want to work with, and who are competent enough to do the work. \n\n# What I Look For\nFor non-tech-specific stuff, the biggest thing I look for is curiosity. Curiosity drives people. A curious developer will dig into docs, dig into source code, and more. The ones who will use a debugger to step into external library code to figure out the issues. The ones who will learn how ICMP fragmentation requests can block network traffic across datacenters. The really great ones, in my experience, just go a couple levels deeper than their coworkers.\n\nNext, I look for how they fit into their individual teams and their scope/responsibility: \n- Were they just taking stories from the [[backlog]] that other people wrote? Were they writing the stories? \n- What portions of the project did they specifically take ownership of?\n- Who designed the system? How did this person work with them?\n- How did the technology stack get chosen, and how did it involve? \n- What were their specific contributions? Outside of the team goals or the product itself, ask for an example of a feature they had the most ownership over, and what it took to drive it to completion\n\nFinally, I have a \"No Assholes\" policy. The days of putting up with rude and abusive people because of their technical competencies are largely gone. You're going to spend a LOT of time with this person. Why make yourself miserable? Some deal breakers: \n\n- Yelling or threatening team members\n- Undermining team members\n- Publicly devaluing team members or contributions\n- Refusing to do tasks they see as \"below\" them\n\nThese people will drive away your best developers, who likely get headhunted on a regular basis. \n\nI want folks who are humble, who are accomodating, who are sympathetic and empathetic. The idea of \"strong opinions, loosely held\" is key. Being rigid or inflexible despite new, contradictory evidence is a big red flag.\n\n# Candidate Considerations\n- The candidate is likely nervous, anxious\n- The desire to get a job if currently jobless can enhance this anxiety\n- Allow candidate time to calm down\n    - Start with easy, intro problems, then expand. \n- Ask them to highlight their background for a few minutes first. Helps get them comfortable talking, and to establish a rapport around their specific technical expertise.\n\n# Technical Interview\nWe all (should) know to avoid the \"gotcha\" questions -- the types where the answer is obvious if you know it, but difficult to surmise if you do not. We're not trying to trick them, we're trying to get an honest assessment of their skill levels. \n\nI do believe there is value in whiteboard coding, but it can't be syntax-perfect. Pseudo-code will often suffice, since what I'm really looking for is the bounds of their competency. Whether or not they miss a semicolon is irrelevant. \n\nI think there is significantly more value in a technical pairing interview. Just a few years ago, we accomplished this with Google Meet, Zoom, or Skype. Nowadays we can just use HackerRank or other such tools. We do a screen share, with the candidate driving, and do some technical stuff, for example: \n\n- I will provide them with a problem and a test harness, and ask them to red-green test their way out. \n- Candidate will present a project (ideally public like on github), and I will ask them to do a show-and-tell. I'd be looking for automated tests, code quality, structure, and documentation, as well as other things\n- Using a system like HackerRank, pair on a programming exercise. HackerRank works well here, since it allows us to let the candidate use whatever language they feel most comfortable in. \n\n\n# Selected Interview Questions\nHere's a few of my favorite questions. \n\n## FizzBuzz\nI will often warm candidates up with the classic FizzBuzz test. It's a relatively well-known problem, stated as: \n\n> Print integers of 1..N, except:\n> - Print \"Fizz\" if an integer is divisible by three \n> - Print \"Buzz\" if an integer is divisible by five\n> - Print “FizzBuzz” if an integer is divisible by both three **and** five.\n\nThe first few in the sequence: \n\n```\n1, 2, Fizz, 4, Buzz, Fizz, 7, 8,  \nFizz, Buzz, 11, Fizz, 13, 14, FizzBuzz\n```\n\nI like this question because: \n\n- It requires the use of a looping construct.\n- It requires the use of conditionals\n- It requires a special case consideration for the \"FizzBuzz\" element\n- It is relatively well-known\n- It is often sufficient to shake off nerves before we get to the real questions. \n\nIdeal candidates will bust this out in a few minutes. Failing candiates will struggle with this question. \n\nIf the candidate can't write a loop construct or the conditionals, in the language of their choice, it's a big red flag as to the level of their experience and actual hands-on proficiency.\n\n## Software Priorities\nIn terms of (some software project), how would you stack rank these competing priorities:\n\n1. Maintainability\n2. Observability\n3. Correctness\n4. Performance\n5. Reliability\n\nObviously they're all important, but the answer must change with the specific project. Medical devices, for example, may care more about reliability and correctness than observability. A GPU rendering pipeline might care about performance to the detriment of all other considerations. A web service might care more about observability and maintainability.\n\nThis is a good base for discussions on software architecture and cross-cutting concerns from a platform/systems standpoint. \n\n# As the Interviewee\n- Remember that these folks want to hire you. Every single person you talk to wants the role filled. They're going to pay someone actual money to do this job, why not you? \n- Explain your thinking. It's ok to take a minute to think, but the interviewer may not know if you're just thinking, or if you are stuck or getting flustered.\n    - If you do get flustered, it's ok to ask for a quick reset. \n    - Practice this with your Yellow Duckie. Talk to it like you're explaining what you're doing to a junior dev who's looking over your shoulder. \n- Be honest about your experience and contributions. It's ok to say \"I don't know.\"\n- It's ok to admit you'd have to look something up online. The interviewer likely doesn't expect you to remember every piece of syntax, but if you can explain what you'd do, that's just as good. \n    - For example, \"given a large file of CSV user data, find all the unique email addresses.\"\n    - \"I'd set the IFS to a comma and select the email address field using awk but I'd have to look up the exact syntax for those. Then we can sort that with the `sort` command, then pipe it to `uniq` to dedupe\" \n    - More important than knowing the information, is knowing that you know how to get and use the information. We all use Google. I have tabs and tabs of documentation open whenever I'm working on a project. We have modern tools like Copilot and Chat GPT. Being a [[Knowledge Worker]] means just that -- you work with knowledge, you don't hoard it. \n\n","n":0.029}}},{"i":54,"$":{"0":{"v":"Continuous Integration","n":0.707}}},{"i":55,"$":{"0":{"v":"Continuous Delivery","n":0.707}}},{"i":56,"$":{"0":{"v":"Automated Testing","n":0.707}}},{"i":57,"$":{"0":{"v":"Architecture","n":1},"1":{"v":"\nConsiderations for best practices\n\n1. Quality Code\n1. Build Automation\n1. Test Automation\n1. CI Pipeline\n1. Deployment Automation\n1. Metrics\n1. Monitoring\n1. Tracing\n1. Observability\n1. Logging\n    1. Structured log output\n        1. JSON, usually\n    1. Request IDs\n    1. User IDs\n    1. A-B test groups\n    1. Feature Flag selections\n    1. Log Aggregators \n        1. ELK Stack\n","n":0.146}}},{"i":58,"$":{"0":{"v":"Articles","n":1},"1":{"v":"Here's a collection of articles I've written over the years. Sometimes they're no longer hosted in their original location, so I am copying them here as an archive.","n":0.189}}},{"i":59,"$":{"0":{"v":"Software Performance 101","n":0.577},"1":{"v":"> Note: This article was written in 2014, and the tools/techniques may have changed. \n\n# Terms\n\nWe are going to discuss how to measure, analyze, diagnose, and ultimately solve performance problems. First, however, we must define what we mean by a \"performance problem.\" There are three major items we talk about when we discuss performance. However, as we all know, that's only part of the picture.\n\n## Latency and Throughput\n\nWhen we think about performance problems, latency and throughput are the two measures that are immediately apparent.\n\nLatency is the time it takes to complete a single operation. Simply put, it's the time taken for some action to complete. Gamers often see this represented as their \"ping time.\" When a webpage is slow to load, that's high latency to the user. A new phone feels faster because the UI is more responsive -- the latency of operations is lower.\n\nThroughput is the number of work items that can be completed in a unit of time. A work item can be anything -- production of a motorcycle, completion of a REST call, or the download of a packet of data. In the world we are talking about, the most common measure we see is requests per second. Essentially, \"how many users can my site support at once?\"\n\n### Intertwined\n\nWhile the measures above are separate in their own way, they are often interwined quite tightly. Imagine a doctor's office, where each visit takes 15 minutes. If there is one doctor, and nobody in the waiting room, you will be seen immediately. If there are 4 people in front of you, you will be seen in an hour. Your perceieved latency is 1 hour. The throughput is 4 per hour. If the office adds another doctor, your perceieved latency is 30 minutes, and the office's throughput is 8 per hour. By increasing the throughput, we have decreased the latency.\n\n## Congestion\n\nWe just learned that by increasing throughput, we can decrease latency. However, that's not quite enough to describe performance problems. The latency above was a fixed number -- 15 minutes. What if the latency was more variable? Some patients may take 5 minutes, other patients may take 2 hours. Additionally, imagine this office has 40 people in it. Your ability to see a doctor is delayed because of the number of people in front of you. This is refered to as congestion (or, if you want to be more computer science-y, queueing delay).\n\n## All Together Now\n\nLet's bring this back to what most of us do: serve webpages. Let's start very simple, with a static HTML page and a server that can handle one request at a time. A single-threaded server handles a request, and returns a page. With little traffic, the latency is quite predictable. When we get more traffic, we start to experience congestion. The average latency to the user increases, and they become unhappy.\n\nTo combat that increase in traffic, we add another thread. We can now handle two requests at a time. With low enough traffic, requests are served immediately. However, the throughput increase is not exactly doubled. There is some overhead to managing threads, dispatching requests, context swaps by the CPU, number of cores, and more. For each increase in parallelization, we get a diminished return in throughput and latency. In fact, it's possible to create so many threads that the system is overwhelmed managing them all, and both throughput and latency are increased!\n\nThis can be further complicated, of course. Imagine a shared resource, like a database connection. In order for the software to work correctly, only one thread at a time is allowed to query and receive data from the database. This introduces yet another possible source of congestion, even if there is no congestion connecting to the web server. These shared resources must be carefully balanced with the rest of the system to optimize performance.\n\n## Debrief\n\nTuning a system for maximum performance is the art of optimizing all three of these parameters with regards to your system. Maybe you don't care about congestion, just that requests are served as quickly as possible once processing starts. Maybe you don't care about latency, and just want raw throughput. Maybe you want no congestion whatsoever.\n\n# Measuring\n\nThere is an old joke in the programming world regarding optimization:\n\n> The First Rule of Program Optimization: Don't do it. The Second Rule of Program Optimization (for experts only!): Don't do it yet.\n\nOptimizing software is tricky business. One thing I've learned over the years is that the problem is almost never where you think it is.\n\n## Math\n\nTo get started, we need to have a quick math refresher. Say that you have an application that reads a REST request, makes some backend calls, and returns a JSON response.\n\nThe amount you gain from optimization is how much the optimized chunk of code is used in the overall lifecycle. If one backend call is responsible for 8% of your runtime, and you are able to 100% optimize it (reduce the time to 0), your overall metric is only seeing an 8% increase.\n\nIn our 8% example, the more likely optimization is somewhere around 20%. Therefore, if we look at it as a function of the total time, we have saved 1.6% on runtime. Imagine a different function is responsible for 20% of your runtime, but you can only optimize it by 10%. This actually works out to be better, as this now speeds up your application by 2%.\n\nWhat you really want to do is identify the biggest pain points, and tackle those first. But, how do you find those?\n\n## Average, Average, and Average\n\nWe all know the term \"average,\" but we need to be more specific when it comes to analysis. Let's say we have a collection of 20 numbers. Each individual number is called a sample.\n\nIf you remember your math, there are 3 things that could mean average: mean, median, and mode. Mode is not generally useful in this scenario. Mean and median are. Mean is what most people are referring to when they say average. The mean is simply adding all the samples up, and dividing by the number of samples. The median is the sample where, if you sorted the numbers, is right in the middle. If your distribution of samples is perfect (like a bell curve), the mean and median should be quite close. When they are not, you have a skewed distribution.\n\n## Percentiles\n\nAverages give you a decent idea of what is happening, but it doesn't tell the whole story. Imagine I told you that your mean webpage response time was 500ms. Seems ok, right? What if we then said that 40% of the people were under 100ms, 40% were under 500ms, and 20% were over 2 seconds. The mean stays the same. This means that 20% of your users have an absolutely terrible experience. We call this a percentile. In much the same way we measure uptimes, there are a few key percentiles we will care about: 95%, 99%, and 99.9%. Most often in a percentile distribution, we are showing the count of items in the set that, when sorted, fit within the percentile bounds.\nStandard Deviation\n\nThis should be the last bit of math. When you take the mean of a set of numbers, you can also calculate a standard deviation. This is the mean distance from the mean for all samples in the set. The smaller the number, the more concentrated your samples. The larger the number, the more variability you have.\n\n## Tools of the Trade\n\nPerformance analysis and optimization is a bit of a blended art and science. The art comes from optimizing the code. The science is how we find trouble spots. In the traditional fashion of science, we want to create a repeatable test. We will run this test frequently, making small tweaks in the code under test, and see how they affect the performance. The key principles in a good test suite are isolation and consistency. Given the same code base, you expect performance results to be comparable between runs.\n\nNow that we know this, and we have our math refreshed, let's talk about some tools we use when evaluating performance.\n\n### Gatling\n\nGatling is one of our favorite tools for performance analysis. It is a high-performance load testing tool, with very pretty (read: management friendly) data visualizations. It is especially good for testing REST-based services. A typical result would look like this:\n\nThere are additional graphs and more fine-grained reporting available.\n\nGatling can also be set up to run as part of your CI server! It has a simple command-line interface, and the project can be maven-ized with this maven plugin. A continuous performance test of your application in a CI environment is fantastic for Continuous Delivery.\n\nWe use this largely as a replacement for JMeter. JMeter is significantly more difficult to get started in, and it's difficult to use without the GUI. Gatling is very well situated for headless operation.\n\n### Yammer Metrics\n\nOk, you have measured with Gatling and you've found something you don't like. How do we take it down to the next level? Yammer Metrics is our best friend for JVM-based applications. This is an AOP-based package that exposes several annotations. Our favorite annotation is `@Timed`, which is a combination of `@Metered` and `@Histogram`. From the documentation for `@Metered`:\n\n> Meters measure the rate of the events in a few different ways. The mean rate is the average rate of events. It’s generally useful for trivia, but as it represents the total rate for your application’s entire lifetime (e.g., the total number of requests handled, divided by the number of seconds the process has been running), it doesn’t offer a sense of recency. Luckily, meters also record three different exponentially-weighted moving average rates: the 1-, 5-, and 15-minute moving averages.\n\nFrom the documentation for `@Histogram`:\n\n> Histogram metrics allow you to measure not just easy things like the min, mean, max, and standard deviation of values, but also quantiles like the median or 95th percentile.\n\n> Traditionally, the way the median (or any other quantile) is calculated is to take the entire data set, sort it, and take the value in the middle (or 1% from the end, for the 99th percentile). This works for small data sets, or batch processing systems, but not for high-throughput, low-latency services.\n\n> The solution for this is to sample the data as it goes through. By maintaining a small, manageable reservoir which is statistically representative of the data stream as a whole, we can quickly and easily calculate quantiles which are valid approximations of the actual quantiles. This technique is called reservoir sampling.\n\nThe best part about these annotations is that they can be in your app at all times. There are multiple ways to get the data out. The normal favorites (JMX, Servlet) are there. In addition, it can also stream data to Ganglia or Graphite to fit into an existing performance monitoring package.\n\n### YourKit\n\n> Note: YourKit is paid, commercial software. While we generally try to recommend free/libre software, this tool is so much better than the OSS alternatives that we believe it is worth the money.\n\nYourKit is a Java profiler. It runs a small agent in your VM, to which a GUI tool connects. The profiler incurs no performance penalty when it is not profiling the application, so it is safe to leave the JAR file in place in production.\n\nThe profiler has 2 ways of measuring: sampling and tracing. With sampling, the tool grabs a snapshot of stacktraces every few seconds, and determines some metrics. This is a very low-overhead method of gathering data.\n\nTracing is significantly more expensive. It will slow your VM by an order of magnitude, but its method tracing is perfect. This is where you want to go to get a deep dive on a method.\n\nSo what do you get for all of this? There's actually too much to talk about in this post. You get memory telemetry (how quickly objects are created and destroyed, what types of objects they are, and where they're created from). You get CPU telemetry, including charts on wall time, your process' time, and time spent in GC, all graphed in realtime. This also includes call stack telemetry. It looks at the path from the beginning of the action all the way down to each individual method. It counts the number of invocations, and tracks telemetry on the rate and latency. This is a fantastic way to identify which processes are taking the longest. In fact, let's look at a screenshot of this method telemetry:\n\nThis is clearly a Spring/Hiberate app interacting with a database. You can see the invocation count, but you can also see the percentages. Those percentages represent the proportion of total time that the method has taken. Luckly, YourKit has also abstracted this into another pane, the Hotspot Identifier. This is basically a no-brains way of determining which methods are taking the most time. This, as we discussed early in the article, will give us the best bang for our optimizing buck.\n\n## The Observer Effect\n\nThis is an axiom in physics that states you cannot measure a system without changing it. This is also true with all three tools above.\n\nGatling can consume a lot of resources. It's a bad idea to have both Gatling and the application under test on the same machine. It's also a bad idea to have Gatling on a machine that experiences variable load, as it can throw off your numbers. Ideally, you have a dedicated load testing machine to drive load.\n\nYammer Metrics has been quite minimal performance-wise, but it does incur some minor overhead. We happen to think that it's worth the small overhead for the insights it gives us, but that's a decision ultimately to be made by your engineering team.\n\nYourKit hooks right into the VM. It incurs the overhead of measuring and monitoring all of the data an reporting it back to the tool. While this tool is mostly useful for tracking down a known performance issue, it can be fun to use it on your application to see if there are any easy wins performance-wise.\n\n## Tying it All Together\n\nWe now have 3 tools, and they work together for us. Gatling gives us a repeatable test runner. It can generate load on-demand at any time. We get a high-level picture of the end-user's experience. Yammer Metrics keeps real-time metrics data on all instrumented methods, and is exposed in a consumable interface. You can even hook up your favorite monitoring system to alert if performance drops. Finally, YourKit gives you the deep insight required to identify the trouble spots in your code base. Once you make a change that looks better in YourKit, you can test it with Gatling to confirm that it makes the end-user experience better, and use Yammer Metrics to confirm the ongoing performance.\n\nNow that we have the tools for measuring  issues, the next step is identifying and fixing the issues, which will be the subject of the next post in this series.\n\n# Causes of Bad Performance\n\nIn the previous paragraphs, we talked about some language and concepts, and then we got into tools and some more math. This has allowed us to identify places we should spend our time.\n\n## Why?\n\nThe first question we ask ourselves when we're tasked with performance enhancements is why? There are a lot of factors to consider, however, there are some basic rules you can start with.\n\nThe first breakdown we have is identifying if the problem is a contention or algorithm performance issue. This maps almost directly to our throughput vs. latency talks from the first section. These are the vast majority of performance issues you might encounter. Contention is what prevents us from getting the throughput that we want.\n\nAlgorithm performance affects latency. Everything works as expected, with little contention, but it's still not fast enough. These problems, while they're more fun to fix, can also mean huge changes. Making a sorting algorithm or a queue implementation or a face detection algorithm faster is an example of algorithmic performance.\n\n> NOTE: While this article is mostly about identifying performance issues in your application, do not overlook the importance of identifying and fixing performance issues in software that your app depends on. Database servers are a common source of issues, for example. You should be running the contention checks on these services if your tests slowness in that area\n\n## Algorithm Performance\n\nSince we are going to spend the majority of our time talking about contention, let's spend a few minutes talking about algorithm performance. I consider an algorithm performance problem one where increasing the speed of the computer is the only way to improve performance. All contentions are minimized, but the code is running into fundamental limits from the hardware.\n\n### Big-O\n\nWhat we are talking about is the basis for Big-O notation. When you see O(n) or O(1) or O(n^2), these are Big-O examples. It helps us figure out the behavior of the algorithm with no restrictions. A merge sort (O(n log n)) is faster than a bubble sort (O(n*n)) because it needs to consider fewer things.\n\n### Data Structures\n\nThis type of performance issue can also arise when you use improper data structures. This is why it's important for programmers to at least be aware of the differences between arrays, linked lists, trees, and maps.\n\n### Caching\n\nThe last bit of note here is one we have all probably done: caching. If you've ever used ehcache or terracotta or memcached or even Hibernate, you've taken advantage of caching. The ability to skip a lengthy computation if the value doesn't change too frequently can mean the difference between 2 and 100 servers to handle the load you want. This can often be used for a quick performance gain if it's your first pass through the application.\n\n## Contention\n\nA computer has a finite amount of resources. Contention is when one of these resources is utilized to its maximum capacity, and there is a queue of work waiting to be done. That resource simply can't handle any more work. Common sources of contention are discussed below, as well as some ways to mitigate the effects.\n\n### Disk Contention\n\nDisk contention is when we spend too much time reading and writing data on disk. Because a spinning disk needs to get to the point on disk where you dsata lives, it takes some amount of time to get data back. If you have a lot of read and write tasks, your request may queue up behind others who haven't been served yet. This will result in long i/o wait times.\n\nTo investigate this issue on *nix-like systems, use the tool iostat. An example of a healthy disk setup:\n\nHealthy Setup\n\nYou can see all the different devices in this example (thanks to Roger for putting it on the internet). The interesting things are the avgqu-sz and await. avgqu-sz is the average size of the request queue. In a mostly-idle system, this will be somewhere around zero, which means all requests are served quickly. In a system with disk contention, this number will grow rapidly. Await is the average time (in milliseconds) that a request took to process -- including queue time and serving time. Let's look at an unhealthy iostat:\n\nUnhealthy iostat\n\nThere are a lot more devices on this machine, but you can see certain devices have a large queue size, and you can see the await times. If you want to serve a webpage in 100ms, but your disk is taking 105ms to service a request, you're going to have a bad time.\n\n#### Blocking vs. Non-Blocking IO\n\nYou've surely heard about node.js by now. It's famous for its non-blocking IO. Whenever your application does something that needs to talk to disk (or something that would cause a wait), it yields to another task, and lets that original task finish later when the data is available.\n\nJava also has some of this. There's a java.io package for blocking IO, and it's probably what most of us use. There's also the java.nio package for non-blocking IO. If you find that waiting for disk becomes an issue, see if you can use non-blocking IO calls to speed up your server's ability to handle more.\n\n#### Reducing Contention\n\nTo reduce disk contention, there are four common strategies. First, you can buy your way out. Solid-state drives (SSD) are significantly faster than spinning disks for random seeks. If your access pattern is a lot of small reads and writes, this will get you very significant benefits in performance. If, instead, your performance problem is one of reading and writing very large chunks of data, an investment in an enterprise storage system may be warranted. A RAID setup combines multiple disks into one, providing (usually) both fault tolerance and increased throughput, since multiple drives contribute to the end result.\n\nThe second common strategy is to simply reduce the amount of data that is on the disk. If you can get by with smaller files, that may increase speeds. Compression can be used on large files, as expanding it in-memory can be faster than reading uncompressed from the disk. A different serialization format can result in significant disk savings (JSON, for example, is quite verbose compared to CSV).\n\nThe third common strategy is to do batch reads/writes. This is what your hard drive controller does, which is why it's so dangerous to just unplug your computer. There is a disk cache that waits for some amount of data, then writes it all to disk at once. This is how some high-performance NoSQL engines work. They keep as much working data in memory as possible, and flush to disk periodically.\n\nThe fourth common strategy is to do more caching. Your operating system likely does some of this for you, by caching frequently-used files in memory. Your application can do the same. If the data on disk changes infrequently, you might want to read it to memory when your app starts up. If it's very large, and you do a lot of searching, see if you can index the file, or sort the file, so you can use faster algorithms like a binary search to get to your data.\n\nThis problem can be magnified if you use disk stores backed by networks. NFS, NAS, Samba, SAN -- these are all disks backed by network. While they may offer unparalleled data security and storage capacity and data mobility, you incur some overhead since it needs to communicate over a network. That leads us to our next step...\n\n### Network Contention\n\nNetworks have a lot of the same issues we have been discussing (latency, throughput, queueing). There's also the issue of network card capacity and network connection capacity. Most servers these days should have a gigabit ethernet card, and high-end servers should be using 10gb ethernet. But if your switches/routers are only capable of 10mbit, you will have a problem.\n\nTo look for potential network issues, there are two tools that are useful. First, there's the venerable netstat. This tool is useful to inspect network connections on the server. This can help you diagnose if you need additional threads to handle connections, if you're leaking connections, or if your system is being overwhelmed by connections. Second, there's a little utility called iftop. It periodically tracks the throughput of all network devices. It can drill down to a single connection, aggregate all connections, and track peaks and means. If you have a 100mbit network card and see ~10MB/sec in iftop, there's a good chance you're maxing out your network. Here's a little screenshot of it running with -t (to just print text to the console instead of an interactive app):\n\n### iftop\n\nSolving network issues can be tricky business. Lucky for us, it's also probably not the source of problems. I have seen it be the source of problems one or two times. In those cases, if you took a thread dump of the process under load, you would see multiple threads blocking on a process that is waiting in the function socketRead0 or socketWrite0 -- native code that actually does the socket communication. Sadly, thread dumps are the most reliable way I've found to differentiate between network issues and other forms of contention.\n\n## CPU Contention\n\nCPU contention is fun, in a way. We all know the command 'top' to display system load and running processes. In the modern world, with lots of threads and processes, it can be helpful to get a bit more detailed in our analysis. The tool htop breaks down each core's load, and provides a lot more than the default top:\n\nhtop\n\nNotice how all 4 cores are showing a high level of usage in this screenshot. I highly recommend this tool to verify that your CPU is overloaded.\n\nWhile this is one of the easiest metrics to inspect, it can be very difficult to fix. The system just has too much to compute to do much more. This is where algorithmic fixes and better data structures come into play. If you notice the system bogging down but the CPUs are not maxed, you have some other contention slowing it down.\n\n### System Load\n\nThe load average from the above screen shot shows the 1-, 5-, and 15-minute averages of load. The load average is an exponentially dampened snapshot of running and runnable processes correlated with wait queue length. With multi-core machines, it gets even trickier. If it was truly just CPU, a 4-core machine with a load average of 1.00 is ambiguous. Is that 1 core fully maxed (and thus ripe for paralellization), or all 4 cores running at 25%? Maybe 2 cores at 50%? With multiple cores, a load average of 4 doesn't necessarily mean the system is under any stress. This is where tools like htop above help diagnose the issue\n\n## Memory Contention\n\nMemory contention is when there is more memory being used than available on your system. Thanks to swap partitions, this shouldn't crash a machine. However, it's likely to destroy your performance. The more tuned your app is, the more detrimental swapping will be. Memory contention can also come when you run into out-of-memory (OOM) problems, or issues with garbage collection in managed apps.\n\nSwap-based issues are easy to spot, as top or htop will tell you the amount of swap they're using. For a production system, you ideally want no swap used. Putting 128GB or more on one machine is perfectly doable these days.\n\nOut-of-memory usually surfaces as a process that dies unexpectedly with no messages. The only way to fix this is to consume fewer resources. This, again, is where better data structures or more compact object representations may help. You may also have memory leaks.\n\n### Garbage Collection\n\nGarbage collection tuning, especially in Java, is almost a job unto itself. There are a lot of tunable parameters. There is the permgen vs. the heap. There's the issue of heap resizing vs. fixed-size heap. When you profile your Java app, you really want to see the classic saw-tooth pattern:\n\nGC-Sawtooth\n\nThis is a generally healthy system. The garbage collector kicks in and returns the heap to about the same size. A more unstable or growing-memory system looks like this:\n\nGC-Bad\n\nNotice how the \"free heap\" blue line climbs, then eventually drops, despite garbage collection happening on the green line. When the free heap is near zero, Java will spend a lot of its time trying to free up the heap, including multiple stop-the-world pauses. These can range from a second or so to multiple minutes, depending on heap size and object counts. If you let it run like this long enough, it will probably become unresponsive and eventually crash with an out-of-memory error.\n\nTuning the GC is beyond the scope of this article, but it can dramatically improve performance.\n\n### Memory Leaks\n\nThe last thing to look out for is memory leaks. Technically it's impossible to do this in Java if you're not using unsafe libraries, but in practice it's a problem. The biggest issue is dangling references to objects that you no longer need. It keeps these objects around in perpetuity, and that could eventually kill your heap. Using tools described in the previous article (visualvm, jprofiler, yourkit), you can inspect where these objects are created, which ones take the most space, and what types of objects they are. This can be very helpful in tracking down excessive memory usage.\n\nThere is a fascinating issue in Java that has cropped up occasionally. If you have a very large string (say, a JSON or XML document, or a large text file you've put into a string), then take a substring of it, that substring is just a windowed view of the larger text string. This is good when your strings are small, as it prevents a lot of reallocation. However, when the source is very large, your substring holds a reference to that large document, meaning your memory usage is far larger than normal. This \"leak\" was fixed in OpenJDK 7u6. If you're still on JDK 5 or 6, you're probably being affected by this.\n\n## Lock Contention\n\nThis is where things get really tricky. I've also found that it's the source of a lot of issues, so it's good to get well versed in this kind of contention. Because multiple threads accessing the same resource (variable, array, database connection, etc) can stomp on each other, there needs to be a way to control access to these sensitive bits. In Java, we often put synchronized on the method definition, or put a critical block inside a synchronized block. Behind the scenes, it creates a lock so that only one thread can execute here at a time.\n\nWhen there are a lot of processes vying for the same lock, you are slowing them all down. If your locks are implemented as spin locks, it may exhibit 100% cpu usage while it waits. If you take a stack trace, you will see \"Waiting on 0xXXXXXXX\" for threads that are waiting for a lock. A useful tool for some users is TDA, and this is how it presents there:\n\n### Thread Dump\n\nNot all locks are deadlocks. Deadlocks are two threads waiting for locks that the other holds, with no way to make progress. Most profilers offer tools to automatically detect deadlocks, and will make your life significantly easier.\n\nLock contention is so expensive that multiple languages/frameworks/patterns are designed to avoid it. Functional programming languages often avoid this issue because they do little-to-no variable mutation. Immutable data structures simplify your life significantly. Because they can't change, it's safe for multiple threads to access the data. Lock-free queues are popular in some circles, but they can be nasty to code up correctly if you're not extremely well-versed in this specialty.\n\n### Mechanical Empathy\n\nIf you're going the ultra-high-performance route, there's a concept you should be aware of called mechanical empathy. This describes a way of organizing your parallelism to minimize the burden on the CPU, especially context-swaps. When you're trying to design sub-millisecond responses in a managed language, it can be difficult to achieve using traditional methods.\n\nMechanical empathy was invented and popularized by the LMAX Disruptor. From the PDF of the whitepaper, this graphic comes to explain the cost of locks:\n\n#### Lock Cost\n\nYou see that as contention (and thus arbitration) increases, your throughput gets tanked. LMAX Disruptor is an attempt to design a system to minimize locking and minimize expensive context switches.\n\nThis is not a simple implementation, and it's not a drop-in replacement for, say, ConcurrentHashMap. Read the link above to get a lot more information and see if it's the right approach for you.\n\n# Conclusion\n\nThis wraps up our tour of performance analysis and tuning. Becoming proficient at analyzing and tuning the JVM as well as analyzing and tuning your own applications, you will have a much deeper understanding of your own code, as well as a much deeper understanding of how the JVM works, which might make for better code. Go forth and profile!\n","n":0.014}}},{"i":60,"$":{"0":{"v":"Myth of Developer Productivity","n":0.5},"1":{"v":"\n> Note: This article is still currenly hosted at https://nortal.com/us/blog/the-myth-of-developer-productivity/. Nortal is a former employer of mine. \n\n**If there is a holy grail (or white whale) of the technology industry, especially from a management standpoint, it’s the measurement of developer productivity. In fact, there is a very common phrase, “you can’t plan if you can’t measure.” Measurement works so well in many other industries that involve humans — building construction, manufacturing, road work. We are able to get rather accurate estimates for both cost and completion date, so why not software?**\n\nIf you’re a manager, you’re going to read a lot of discouraging information here. However, if you make it to the end, I promise we’ll give you tools and tips to gain efficiencies. All is not lost.\n\n# Why measure?\n\nWe as developers love to play along with this. So much of what we work with is data-driven feedback. We can analyze with profiling, complexity, conversion rates, funnel metrics, heat maps, eye-tracking, a/b testing, fractional factorial multivariate analysis, etc. All of these things give us data upon which we can prioritize future efforts. It only makes sense that we should be able to measure ourselves.\n\n# Measuring developers\n\nMeasuring and managing developer productivity, however, has consistently eluded us. So many of the tools we use are designed to increase developer productivity: XP, TDD, Agile, Scrum, etc. There were academic papers analyzing software project failures/overruns in the 80s. This isn’t a new phenomenon by any means. We also famously hear of IT failures in the news, such as:\n\n- (2004) – UK Destroys Tax Records, costing at least £85m.\n- (2004) – Ford and Oracle scrap Purchasing System, costing $400m\n- (2007) – FBI Virtual Case Files Scrapped, costing $170m\n- (1962) – Rocket Failure for Missing Hyphen, costing $135m in today’s dollars.\n\nThese are just a few cases. There are likely dozens or hundreds of errors on this scale every year, and likely hundreds to thousands of projects in the <= $1m range. A lot of this is due to a lack of good testing. We at Nortal have frequently espoused the benefits of automated testing, and it has real benefits.\n\nHowever, quite a few others are caused by planning and estimation that missed the mark. There are estimates that say IT organizations will spend over $1t per year on their IT initiatives. Notice it’s trillion, not billion. A trillion dollars. Given this extremely high cost, anybody who found a way to reliably gain efficiencies of even 1% would save a billion ($1,000,000,000) dollars. That’s a lot of zeroes.\n\n# The 10x developer\n\nThere is a theory floating around, and largely backed up by data, that the best developers among us are 10x more efficient than the worst ones. Given that developer salaries do not reflect this order-of-magnitude difference (Who is the last senior dev you knew who made $800k/yr?), it’s obviously a bargain for companies if they can find one of the 10x, and hire them at a comparable rate to a 1x or 2x person. These studies even gave birth to analysis that showed, “…[T]he top 20 percent of the people produced about 50 percent of the output (Augustine 1979).” If you were a manager looking to cut costs, you’d want to get rid of 80% who produced only 50% of the output, and hire only the kind of people who are in that top 20%.\n\n## High performers\n\nHowever, that quote I gave you is not the full quote. It actually is:\n\n> This degree of variation isn’t unique to software. A study by Norm Augustine found that in a variety of professions–writing, football, invention, police work, and other occupations–the top 20 percent of the people produced about 50 percent of the output, whether the output is touchdowns, patents, solved cases, or software (Augustine 1979).\n\nThis problem is not a software-specific problem. Any field that requires human decision-making is subject to variation. Some people are going to be naturally talented in the field. Some have the perfect personality for the job. Some people are voracious readers, others never try to learn after school. Some consistently push their bounds, while others are content to be competent. Some people’s brains just work differently. Some people’s bodies just work differently. It doesn’t take a genius to see that some football/soccer/hockey players are dramatically better than others, even though they both train the same amount of time. Why would software development be any different? Why should it?\n\n# Traditional measures\n\nBefore we continue onward, let’s look at some of the ways the industry has tried to quantify development activities, and why they fall short for measuring productivity. The tl;dr of this section is that any metric you come up with to measure developers will be gamed.\n\n## Hours worked\n\nThis is one of the most obvious ones: butt-in-seat time. If you worked 10 hours instead of 8 hours, you should get 125% of the work done. That’s just math. Time and time again, you’ll see studies proving that this just does not work for anyone. In fact, running hot on hours is a great way to decrease productivity.\n\n- The Relationship Between Hours Worked and Productivity (Stanford)\n- Henry Ford Drops Hours, Increases Productivity\n- Stop Working More Than 40 Hours Per Week\n\n \nTime and time again, we see proof that more than 40 hours necessarily leads to a drop of productivity, even for assembly line workers. Yet, this pervasive attitude of 8-6 being a minimum workday continues to chug along.\n\nI was once on a team where the managers were so addicted to tracking hours as a measure of productivity that we started putting meetings, lunches, and bathroom breaks on the board every sprint. Otherwise, we were accused of not working hard enough because our hours didn’t exactly add up to 40 or more. This absolutely destroyed the morale of the team. “Don’t forget to put your hours in” causes me to involuntarily twitch.\n\n## Source lines of code (SLOC)\n\nLines of code. What a perfect measure. Even if they think different and whatnot, we can just track lines of code, and use that to extrapolate.\n\nThere are so many problems with this metric that it is actively harmful to use it to judge developers:\n\n- Developers can just add extra lines of code to pad their numbers\n- A 200-line solution may be faster or more performant than a 1000-line solution to a problem\n- Sometimes the solution is to delete code\n- 5000 lines of buggy code is worse than 1000 lines of bug-free code.\n- Developers copy-paste code instead of refactoring, leading to massive technical debt and poor design, as well as significantly increased bug probability.\n\nThis is an interesting metric to track in aggregate to get a sense of the size and complexity of the system, but not useful at an individual level.\n\n## Bugs closed\n\nThis one is so crazy, Dilbert has a comic on it:\n\n## Function points\n\nFunction points found a small following out in the world. You’ve probably never heard of them. It’s practically impossible for a lay-person to digest. If you want to try to measure function points for your project, then give this article a read and figure out how to automate it in your project.\n\nGo ahead, try it. I dare you.\n\n## Defect rate\n\nThe idea of this one is to measure the number of defects each developer produces. This does seem reasonable, and you should probably track it, but here’s why it’s a bad measure of productivity:\n\n- It favors bug fixes over feature development.\n- It discourages developers from tackling larger projects. Would you rather try the “Add a form field to this existing page” project, or the “Implement a real-time log analysis system from scratch” project?\n- Not all bugs are created equal:\n    - Bug 1: When somebody uses the “back” button, a bug deletes all customer data on the production website.\n    - Bug 2: Form fields are not left-aligned\n    - Bug 3: If a customer enters dates that span 2 leap years, the duration calculation is off by 1 second.\n- People often mistake features for bugs. Missing requirements are not a bug, but may be filed as such.\n- There may be multiple bug reports related to 1 bug.\n- Developers will never touch anybody else’s code, and will get very aggressive about protecting their code.\n\nDefect rates are interesting, but they’re not enough to give you an idea of productivity.\n\n## Accuracy of estimation\n\nEstimation, my least favorite activity. I have no problem taking a swing a how long something will take. However, at every single company I’ve ever worked for, estimates become commitments. If you say “this will take about 3 days,” you get in trouble if it takes longer than 3 days. On the other hand, if you finish ahead of schedule, you get praised. This encourages developers to estimate given an absolute worst-case scenario. Like, “neutrino streams from solar flares corrupting random bits on our satellite stream that somehow passed checksum validation but is still corrupted and we wrote that to our hard drive” kind of worst-case scenarios.\n\nOther reasons this metric is a problem:\n\n- If you estimate in “ideal hours,” distractions may turn that 8-hour task into 3 days.\n- Developers can be overly and inconsistently optimistic with their estimations.\n- The scope was not adequately defined, or not defined at all.\n- The customer was asking for something that is impossible, which could only have been discovered at coding time.\n\nThere is one more reason, bigger than those four combined. Look for the section “Developer Productivity is a Myth.”\n\n## Story points\n\nStory points — we thought we had found the holy grail. Story points were explained as a measure of effort and risk. If we have consistent story points, and figure out how many story points each developer finishes per sprint, then we can extrapolate developer performance. Let’s see what happens:\n\n- If they finished less than they did last sprint, they’re chastised. They are again reminded that they committed, no matter what. Even if you had to help a prod issue, or were in a car accident, or got sick — you committed. So developers start sandbagging to avoid this.\n- If they finished exactly right, the managers will think the developers finished early and were sitting idle, or were padding their estimates. This leads to frustration and resentment. Alternatively, a perfect finish might be seen as a state where, if everybody worked a few more hours, we’d see more output.\n- If they finish with more points than they took on, managers will accuse the developers of sandbagging. Then they told that they must accept more points next sprint, to take this into account. That, or you have a “level-setting meeting” where everybody re-agrees what the points represent. This leads to frustration and resentment, not to mention the drop in productivity related to figuring out the new point system.\n\nIf a manager asks for doubled productivity, that’s easy: double the story-point estimate.\n\nStory points also aren’t consistent between developers. Even if everybody agrees that it’s a 3-point story, based purely on effort and risk, the wall-time delivery will be different depending on who picks it up. One developer who is intimately familiar with that code may be able to finish in 2-3 hours, while a new junior developer may struggle for 1-2 days. This is proof that we’ve decoupled productivity from points, and why it’s a bad metric.\n\nOn the official Scrum forums, practioners always have to explain why story points are not a measure of productivity. The Scrum Alliance even has a whitepaper called The Deadly Disease of Focus Factors, and here is the opening statement of the document:\n\n> To check your organizational health, answer these two questions:\n>\n> 1) Do you estimate work in “ideal” hours?\n>\n> 2) Do you follow up on your estimates, comparing it to how many “real” hours work it actually took to get something done?\n>\n> If so, you may be in big trouble. You are exhibiting symptoms of the lethal disease of the “focus factor”. This is how the illness progresses:\n>\n> Speed of development will keep dropping together with quality. Predictability will suffer. Unexpected last moment problems and delays in projects are common. Morale will deteriorate. People will do as they are told, but little more. The best people will quit. If anything gets released it is meager, boring and not meeting customer expectations. As changes in the business environment accelerate, the organization will be having trouble keeping up. Competitors will take away the market and eventually the end is unavoidable.\n\nSo even the people who invented the concept tell you explicitly not to use story points as a measure of developer productivity. So stop it.\n\n# Developer productivity is a myth\n\n“You can’t plan if you can’t measure.” This is an idea still taught in business school, it’s a mantra of many managers, and it’s wrong in this context. It assumes everything a developer does is objectively and consistently measurable. As we’ve shown above, there still doesn’t exist a reliable, objective metric of developer productivity. I posit that this problem is unsolved, and will likely remain unsolved.\n\nJust in case you think I’m spouting nonsense, just remember: the smartest minds of Microsoft, Amazon, IBM, Intel, Wall Street, the Bay Area, Seattle, New York, and London still haven’t found that magical metric. It is, therefore, a rather safe assumption that the average company also hasn’t found it. If you believe you have proven me (or them) wrong, go ahead and publish it. You’ll be a wealthy rockstar of the programming universe. People will write books about your life and your brilliance.\n\nWe all know that some people are better than others. Developers can identify which developers are better, but there is not a number or ranking system we can come up with, objectively based on output, that consistently and reliably ranks developers. Let’s explore why.\n\n# A developer’s job\n\nMost people don’t understand what developers do. We clicky-clack on electronic typewriters while drinking Mountain Dew and eating Doritos in the dark, and make the magic blinky boxes show cute cat pictures.\n\nOK, it’s not the 90s anymore. Most people really do understand the basics of operating a computer. If you’re under 40, there’s a good chance your grandparents use Facebook.\n\nSo what do we do? Code is the output, but it’s not really what we do. If we were just transcribing something into code, that’s basically data entry. We’re knowledge workers. We take inexact problems and create exact solutions. Imagine if managers were capable of exactly specifying the system they want built. They would have to explain it so finely-grained that it would be programming. That’s what we do. We are people who exactly detail how a system works. Our code is the be-all, end-all specification for what the software does. We are people that write specifications, digest knowledge, and solve problems.\n\nMost people are incapable of breaking a problem down to the level required for computer code to solve it. This isn’t to say that they can’t learn, but it’s a skill you must nurture. Imagine a parent (P) trying to teach a kid (K) how to make a grilled cheese sandwich:\n\nK: How do you make a grilled cheese sandwich?\n\nP: You make a cheese sandwich, then fry it in a pan until it’s done.\n\nK: What’s cheese?\n\nP: It’s a food made from milk.\n\nK: How do they make cheese?\n\nP: Well, they take milk, and they add rennet, then they add flavorings, and maybe age it.\n\nK: What’s rennet?\n\nP: It’s an enzyme that makes the milk solid\n\nK: How does it do that?\n\nP: It is a protease enzyme that curdles the casein in milk.\n\nK: How does a nucleophilic residue perform a nucleophilc attack to covalently link the protease to the substrate protein before releasing the first half of the product?\n\nP: Because I said so.\n\nImagine the plethora of questions they can keep asking: How do you tell if it’s done? What does done mean? How many minutes? What’s a minute? Why is a second a second and not something else? How brown is too brown? What kind of bread do you use? How do you make bread? What is bread yeast? What’s butter? What’s a pan? How do you make a pan? What’s a stove? Why does a stove get hot? How does a stove get hot? What happens if you don’t have cheese? What happens if you don’t have bread? Can you use a microwave? Can you cook it outside if it’s really hot? Can you use other cheeses?\n\nSo when somebody in the business asks, “can you tell me how many people visited our site yesterday and clicked on the newsletter signup?”, it sounds like a simple request. You just take all the people, find the ones who clicked the thing, and count it. But, let’s take a dev perspective. How do we identify visitors? Is IP good enough? Do we support IPv6? Do we want to use cookies? Is our cookie policy legally compliant in Europe? Do we have to worry about COPPA? Do we want to de-dupe visitors? How do we track that people clicked on a link? What’s the implication of click-stream tracking? Will our infrastructure support that? How important is accuracy? If we lose one click record, does that matter?\n\nThis is what developers do. For every line of code we write, we are answering all of these questions in excruciating detail.\n\nWhen you hear developers talk about “abstraction,” we are basically answering the “How does electricity get turned into heat?” question for anybody who asks. Then we’re answering the “how does a protease enzyme curdle casein?” question. Then we’re answering the “how does heat turn bread brown?” question. One of the questions we literally answer is, “How do you turn 1s and 0s into text?” Well, what about character encodings or code pages or multi-byte entities or byte-order markers or little-endianness… you get what I’m saying. A computer is a dumb machine. It can’t read our minds, and has no context.\n\nA good developer is able to take a high-level problem, see best way to break it down, and create the correct levels of abstraction, all while keeping the code readable and maintainable for other developers. This also explains why some people are 10x performers, and some people get so frustrated with programming that they give up. Some people have curated, or have a natural talent for, thinking at this extreme level of detail. Some people can intuit things that others will never discover — even if they had all the time in the world. This is the nature of knowledge work.\n\n# Professionals\n\nThis one is likely to be more controversial, but the crux of this issue is that developers are often treated like blue-collar workers. Because so many of our beloved processes come from the world of manufacturing, it’s very easy to see why developers would be though of like assembly line folks. That’s why managers try to get consistent productivity. The idea is that if they can just find a way to measure developers, then developers will truly be interchangeable cogs: software would never be late again, it would always be on budget, and it would be exactly what we want. All of the theory they learned about manufacturing and assembly lines in business school would then apply to this field.\n\nThis attitude led to the massive amounts of off-shore outsourcing, just like manufacturing. These days, we know that offshore development is very difficult to get right, the end product often contains a lot of bugs, and is often of very poor quality. Many companies are bringing off-shore projects back in house due to these issues — or using local consulting firms like Nortal.\n\n# What about those builders?\n\nSo what makes building and road work so predictable, when we can’t get it right for development? The answer is relatively simple: we’re not doing the same job. The labor in those fields have very little input on the decision-making processes. As we explained above, what a developer does all day is make thousands of tiny decisions. By the time these construction projects break ground, the decisions are made, the plans are already in place, there are very exact specifications, and there is little room for ambiguity or disagreement. In addition, the skills required aren’t as widely variable. One person can use a pneumatic nailer about as good as any other. One person can operate a dump truck about as good as any other. And even if somebody was a 10x better paver than another, the time needed to cure is a near-constant factor. In addition, the tools and techniques are not as rapidly moving. The basics of foundations, jack studs, jamb studs, nogging, top plates, mudding, and taping really hasn’t changed. Governments and building codes will dictate many of the decisions, like how far apart studs are center-to-center, or how many electrical outlets go on a wall.\n\nRather than trying to build an analogy to builders, who makes all the decisions? City planners, building code authors, architects, and engineers. All while dealing with a highly beaurocratic permit system, and localities that have different rules. They make tons of decisions.\n\n# Professionalism\n\nLet’s do another thought process. If developers were truly thought of as professionals, let’s see how other professions compare.\n\n## DOCTORS\n\nAsk a doctor what their job is. Is it talking to people? Is it writing prescriptions? Maybe it’s taking inexact problems from imperfect people with imperfect information, then trying to diagnose and fix or ameliorate problems, within the constraints of cost, time, side effects, and a million other things. Sound familiar?\n\nSo how do you measure the productivity of doctors? Given their high cost, obviously the field should be rabid for productivity optimization, right? Doctors have something called RBRVU, or “Resource-Based Relative Value Units.” From that article:\n\n> […] if your organization is measuring physician productivity based on how many patients a doctor sees per day, it needs to take many relativities into consideration. If you compare a primary care physician with a small practice to an ED physician, you are unlikely to see a day when the PCP sees more patients than the bustling ED physician – but is that really a fair and accurate measure of productivity? However, within your organization, if you stack doctors up against those in like-practice, thinking that you can judge productivity on numbers alone, you run into the trap of complexity of care – even within the same speciality, practices may be saddled with patients in varying degrees of medical complexity – and even that will change over time within the same patient!\n\nThis seems rather familiar.\n\n## Lawyers\n\nOk, let’s try lawyers. Is their job reading briefs? Is it writing them? Is it consulting with people? Or is it doing all of that, while interpreting imperfect laws with imperfect information based on second- and third-hand reports of a situation, while absorbing all of the decisions of the past?\n\nWe all are pretty familiar with the traditional method of measuring productivity of lawyers: their billable hour counts. Even there, people are discounting that metric. The only goal of billable hours is higher partner profits. From that article:\n\n> The relevant output for an attorney shouldn’t be total hours spent on tasks, but rather useful work product that meets client needs. Total elapsed time without regard to the quality of the result reveals nothing about a worker’s value. More hours devoted to a task can often lead to the opposite of true productivity. Common sense says that the fourteenth hour of work can’t be as valuable as the sixth. Fatigue compromises effectiveness. That’s why the Department of Transportation imposes rest periods after interstate truckers’ prolonged stints behind the wheel. Logic should dictate that absurdly high billable hours result in compensation penalties.\n\nHey, there’s something interesting. “Useful work product that meets the client needs.” How does Scrum define success? Value delivered to the business. It says nothing of how you determine that value. There are too many factors. It may even be impossible to directly correlate revenue to features. Therefore, the only measure of success in scrum is that the product owner is happy.\n\n## Developers\n\nSo those two fields, often considered where the best and brightest go, have found that hours and other obvious metrics aren’t useful to measure productivity. So, why aren’t developers treated the same way? Why do we keep being excluded from the “Professional” list?\n\nI’m not suggesting any solution here. I just don’t have one. However, it helps explain things like calling developers resources. From that article:\n\n> Does George Steinbrenner schedule a “short stop resource” or does he get Derek Jeter?\n\n> Do they Yankees want homerun hitting A-Rod or a mere “3rd baseman resource”?\n\n> Did the Chicago Bulls staff a “shooting guard resource” or did they need Michael Jordan?\n\n> Did Apple do well when it had a CEO “resource” or did they achieve the incredible after Steve Jobs came back to lead the company?\n\nThoughtworkers and creative types are no different. Software engineers are simultaneously creative and logical, and there is an order of magnitude difference between the best and worst programmers (go read Peopleware if you don’t believe this). Because of this difference, estimates have to change based on the “resource,” which means we’re not interchangeable cogs after all.\n\n \n# You promised me tools!\n\nSo let’s assume that measuring — or more importantly, optimizing — productivity is nearly impossible. How do you keep your team happy and still satisfy the business need for efficient use of capital? Well, what do these other professionals do? Instead of trying to directly measure productivity, they measure anything that impedes productivity.\n\n## Measuring impediments\n\nThis is an easy one. Every time something impedes progress, make a note of what it is, and how long it took to resolve. This is especially good to do for any external dependencies. Any time the work leaves the direct, in-progress control of the developer, track who it goes to, and how long they have it.\n\nYou can then use this information to talk with the external groups. For example, if the IT folks are taking 2 weeks to turn around a virtual machine, that’s a discussion the Dev manager can have with the IT manager. If you have a policy of mandatory code reviews, then track that time. Maybe people are letting those sit around for 3 days, and the manager can set priorities. Maybe there are competing priorities. Either way, the dev manager can show THEIR boss why work items are taking longer than they need to.\n\n### Time before delivery\n\nThis is another interesting metric. Track how long it takes from the point the business requests a work item, to when it’s available for use in production. Over time, this metric will stabilize. If the units of work are somewhat consistently sized, predictability will be gained.\n\n### Time in progress\n\nThis one tracks the total amount of wall time taken from when work starts on an item, to when it’s delivered. Again, if the units of work are approximately similar sized, predictability will be gained here.\n\n### Time in phase\n\nThis one tracks the wall time in each phase. Remember how I told you to track external organizations? You should be tracking every phase. The design phase, the dev phase, the QA phase, the code review phase, even the deployment phase. By having every phase tracked, you can identify the slower phases, and see if there is any room for optimization.\n\n### Flow control\n\nJust like working more than 40 hours leads to less productivity, so does working on too much at once. There’s a rule of optimization that you can optimize a process only as much as you can optimize a stage. The way to get more done is to remove bottlenecks.\n\nIf the QA team is only able to test 4 stories weekly, but developers are finishing 10 stories per week, then only 4 features per week are going to be released. Speeding up the developers will have no effect on the number of features delivered per week. You have to get the QA team to get more throughput. If the managers didn’t know the QA team was the bottleneck before, it’s impossible to ignore the pile of work that’s growing in their phase.\n\nTo this end, it makes sense that instead of developers taking a bunch of items on at once, they should focus on one item, and drive it to completion. In addition, there should be some limit of total features being worked on at one time. Work that’s being done beyond what the QA team can handle is wasted work. If your developers can help resolve the roadblock in the QA queue, that’s going to deliver more value to the customer than working on features. And if we forgot, value is the true output we’re trying to deliver.\n\n# Wait a minute…\n\nIf you think this all sounds a little familiar, it should. It’s the basics of Kanban. It again comes from the manufacturing world, but the focus is on a continuous delivery of value to the customer, with a minimum of wasted work.\n\nThe basics of Kanban:\n\n- Map your value stream. This means separate stages for any handoff point. This also should include any external factors that might impede progress. Then you track the time a story spends in each phase, as described above.\n- Define the start and end points for the Kanban system. Some teams find if valuable to have To-Do, Doing, and Done. Some teams have Backlog, Design, Dev, Code Review, QA, Release, and Done. It’s up to you. Anywhere there’s a political or team boundary is a perfect place to have a new phase.\n- Limit WIP (Work In Progress). As we explained above, increasing productivity of developers without clearing the downstream bottleneck results in wasted work, and no adiditional value delivered to the customer. The team shoul agree on WIP limits, and situations which might allow for breaking those imits.\n- Service Classes. We know that some production issues will have to take priority. You can have different classes of service (e.g. “standard”, “expedite”, “fixed delivery date”).\n- Adjust Empirically. Given the data you’re tracking above, you can find bottlenecks and inefficiencies, and work to resolve them.\n\nThis is the current best solution we’ve found. Instead of trying to directly measure programmer productivity, which we showed above is practically impossible, focus on measuring anything that impedes their progress, or the progress of delivering value to the customer.\n\n \n# Intuitions\n\nFinally, a little note for you, which is often the antithesis to empirical measurement: trust your gut. Even though you can’t just put numbers on it, most developers find it easy to spot good and bad developers. There’s just something telling you that they’re better. It could be the way they talk about their technology, the thought they put into an answer, or the answer itself. Most developers would sacrifice project and pay to work with a former favorite co-worker. Managers, if you have a developer you like and trust, then trust their input on their coworkers.\n\nIn addition, even though they may not be developers, managers often already know who their best and worst performers are. There’s usually one or two standout people, even in a team of already-amazing people. If you have all of your developers stack rank each other, it’s likely the top performs and the worst performs would be quite consistent. This doesn’t fix the issue of finding or hiring developers. The troubles of interviewing could be the subject of an article even longer than this one.","n":0.014}}},{"i":61,"$":{"0":{"v":"Java Release Process with Continuous Delivery","n":0.408},"1":{"v":"\n> This is a re-post of an article that is no longer hosted online. You can see it on the wayback machine at https://web.archive.org/web/20141221084323/http://www.dev9.com/blog/2014/9/java-release-process-with-continuous-delivery\n\n## Intro\n\nOne of the most interesting things we deal with is releases. Not a deployment -- which is actually running the new software. A release, in our parlance, is creating a binary artifact at a specific and immutable version. In the Java world, most of us use Maven for releases. More pointedly, we use the `maven-release-plugin`. I am going to show you why you should stop using that plugin.\n\n## Why Change?\n\nThis is a question I field a lot. There are several reasons, but the primary one is this: In a continuous delivery world, any commit could theoretically go to production. This means that you should be performing a maven release every time you build the software. So, let's revisit what happens inside your CI server when you use the maven-release-plugin properly:\n\n- CI checks out the latest revision from SCM\n- Maven compiles the sources and runs the tests\n- Release Plugin transforms the POMs with the new non-SNAPSHOT version number\n- Maven compiles the sources and runs the tests\n- Release Plugin commits the new POMs into SCM\n- Release Plugin tags the new SCM revision with the version number\n- Release Plugin transforms the POMs to version n+1 -SNAPSHOT\n- Release Plugin commits the new new POMs into SCM\n- Release Plugin checks out the new tag from SCM\n- Maven compiles the sources and runs the tests\n- Maven publishes the binaries into the Artifact Repository\n\nDid you get all of that? It's **3 full checkout/test cycles, 2 POM manipulations, and 3 SCM revisions**. Not to mention, what happens when somebody commits a change to the pom.xml (say, to add a new dependency) in the middle of all this? It's not pretty.\n\nThe method we're going to propose has **1 checkout/test cycle, 1 POM manipulation, and 1 SCM interaction**. I don't know about you, but this seems significantly safer.\n\n## Versioning\n\nBefore we get into the details, let's talk about versioning. Most organizations follow the versioning convention they see most frequently (often called Semantic Versioning or SEMVER), but don't follow the actual principles. The main idea behind this convention is that you have 3 version numbers in dotted notation `X.Y.Z`, where:\n\n- `X` is the major version. Any changes here are backwards-incompatible.\n- `Y` is the minor version. Any changes here are backwards-compatible, but there may be bug fixes or new features.\n- `Z` is the incremental version. All changes here are backwards-compatible.\n\nHowever, most organizations do not use these numbers correctly. How many apps have you seen that sit at 1.0.x despite drastic breaking changes, feature addition/removal, and more? This scheme provides little value, especially when most artifacts are used in-house only. So, what makes a good version number?\n\n- *Natural order*: it should be possible to determine at a glance between two versions which one is newer\n- *Build tool support*: Maven should be able to deal with the format of the version number to enforce the natural order\n- *Machine incrementable*: so you don't have to specify it explicitly every time\n\nWhile subversion offers a great candidate (the repository commit number), git does not have the same. However, all build systems, including both Bamboo and Jenkins, expose an environment variable that is the current build number. This is a perfect candidate that satisfies all three criteria, and has the added benefit that any artifact can be tied back to its specific build through convention.\n\n### What about Snapshots?\n\nSnapshots are an anti-pattern in continuous delivery. Snapshots are, by definition, ephemeral. However, we're making one exception, and that's in the POM file itself. The rule we're following is that the pom.xml always has the version `0-SNAPSHOT`. From here on out, no more snapshots!\nThe New Way\n\nSo, we're going to use the build number as the version number, and not have snapshots (except as described above). Our POM file is going to look a little something like this:\n\n```\n<project ...>\n  ...\n  <version>0-SNAPSHOT</version>\n</project>\n```\n\nThis is the only time we will use `-SNAPSHOT` identifiers. Everything else will be explicitly versioned. I am assuming your distributionManagement and scm blocks are filled in correctly. Next, we need to add 2 plugins to our POM file:\n\n```\n<build>\n    ...\n    <plugins>\n    ...\n        <plugin>\n            <groupId>org.codehaus.mojo</groupId>\n            <artifactId>versions-maven-plugin</artifactId>\n            <version>2.1</version>\n        </plugin>\n        <plugin>\n            <artifactId>maven-scm-plugin</artifactId>\n            <version>1.8.1</version>\n            <configuration>\n                <tag>${project.artifactId}-${project.version}</tag>\n            </configuration>\n        </plugin>\n    </plugins>\n</build>\n```\n\nThe devil is in the details, of course, so let's see what should happen now during your release process. Note that I am using Bamboo in this example. You should make sure to modify it for your CI server's variables. The process is:\n\n1. CI checks out the latest revision from SCM\n1. CI runs `mvn versions:set -DnewVersion=$ {bamboo.buildNumber}`\n1. Maven compiles the sources and runs the tests\n1. Maven publishes the binaries into the Artifact Repository\n1. Maven tags the version\n\n> Steps 3, 4, and 5 are run with one command: mvn deploy scm:tag.\n\nThat's it. We have one specific revision being tagged for a release. Our history is cleaner, we can see exactly which revision/refs were used for a release, and it's immune to pom.xml changes being committed during the process. Much better!\n\n## Gotcha!\n\nOk, this all works great, unless you have a bad setup. The primary culprit of a bad setup is distinct modules having snapshot dependencies. Remember how I told you snapshots are an anti-pattern? Here's the general rule: if the modules are part of the same build/release lifecycle, they should be put together in one source repository, and should be built/versioned/tagged/released as one unit. If the modules are completely separate, then they should be in a separate source repository, and you should have fixed-version dependencies between them to provide a consistent interface. If you are depending on snapshot versions, you are creating non-repeatable builds, as the time of day you run the build/release will determine which exact dependency you fetch. ","n":0.032}}},{"i":62,"$":{"0":{"v":"Business Value of Automated Testing","n":0.447},"1":{"v":"\n# The Business Value of Automated Testing\n\n> This is a repost of a blog article I wrote a while back for a site that no longer exists. It was originally published Feb 12, 2014. \n\n\n> Modern (Jan 29, 2019) Take: Is this really even a debate anymore? I feel like the whole world has come around to the value of automated testing. It's so fundamental to the modern development workflow that any company not having tests is going to lose market traction to competitors who can meet the market more quickly.\n\n\n## History\n\nModern developers have been exposed to all sorts of automated testing concepts: Test-driven development, unit testing, integration testing, behavior-driven development, functional testing.\n\nDespite this, some people and organizations have a strong aversion to automated tests. Their objections stem from the near-truths \\(\"Our system is too complex to test\"\\), to the probably-false \\(\"Testing doesn't provide any value\"\\) to the completely ignorant \\(\"I don't know what a unit test is\"\\). In this article, we will make the case that automated testing provides business value at all tiers, and no matter how much you do, you're probably not doing enough.\n\n## Redefining Terms <a id=\"redefiningterms\"></a>\n\nForget the battles between emacs and vim and the wars of Microsoft and Linux. The real battle line of modern developers is the definition and distinction between unit tests and integration tests. Never has such effort been put into drawing ever-finer boundary lines between these layers of testing. In the end this distinction is ultimately meaningless.\n\nHistorically, unit tests are supposed to depend on nothing except the class or method under test. Developers mock or stub all external actors, and then test just the code you want to test. Integration tests referred to the testing of the application where some of the external systems are available and working. A classic example of this in the Java world is testing your JDBC connection strings. It's great to unit test them, but until you try to connect it to a live database, you don't actually know that it works for sure.\n\nInstead of giving you our definition of these terms, we're going to focus on a different set of terms. Commit-, functional-, and acceptance-level tests.\n\n### Commit Level Tests\n\nCommit level tests happen, not surprisingly, when you commit the code. They are the first line of defense against breaking changes. These tests can be unit or integration, but the key feature of these tests is that they finish quickly. The entire commit level test suite should run in under 5 minutes. This keeps the feedback loop tight, and allows the continuous delivery pipeline to fail as quickly as possible.\n\n### Functional Level Tests\n\nFunctional level tests happen after the commit level tests happen. They are the gatekeeper between an artifact and promotion to the next environment. The tests here can take as long as needed to verify the software, but obviously, faster is better.\n\n### Acceptance Level Tests\n\nThese tests happen after the software has been deployed into an environment. They operate on the application to verify that everything is working as expected. Smoke tests are an example of acceptance-level tests.\n\n## Validation and Verification\n\nThese two terms are used interchangeably, but they are drastically different in the formal world of tests and proofs. Validation checks that the software meets the high-level requirements from a user's standpoint. It is validating that the requirements were correct in the first place. Verification means that the application was developed to the requirements appropriately. In short, validation is \"building the right thing,\" and verification is \"building the thing right.\"\n\nCritics of automated tests often claim that it's impossible to prove the software is working correctly, so it's a waste of time. While they're correct in the first part, they're very wrong on the second part. No software, outside from a group of people whose work is largely research-oriented, is provably correct. Microsoft Windows isn't provably correct. Neither is Linux. Even Java isn't provably correct. Additionally, any formal proof of software correctness is only as good as the proofs themselves, and is dependent upon a formalized proof of the hardware correctness as well. Also, manual testing can't prove the software is correct. The inability to prove the correctness of software is, in our current world, a truism. It's also irrelevant.\n\n## The Value Proposition\n\nGiven that we understand automated tests can never prove software correct, why do we still do them? Let's make a bold claim: automated tests are not for software validation or software verification.\n\nA lot of developers who practice TDD follow the red-green approach. They write tests, run them, and see the red lights. Then they build software, re-run the tests, and get a green light. That means the code you wrote works, right? Nope.\n\nWhat you have done is codify a behavior. Given a certain set of inputs and the software as it was written, you have a certain set of outputs. You have a small chunk of _behavior_ validation.\n\nWhy do we make changes to software? It's usually to change the behavior of the software. Adding or removing a feature is changing the behavior. Fixing a bug is, in fact, changing the behavior. It was incorrect behavior, and that's why you write a new test to make sure the behavior never reverts accidentally.\n\nIn the process of changing the behavior of a system, you might have to make large changes to the software architecture. Given the potential scope of changes, how can you be reasonably sure that the _behavior of the system hasn't changed_?\n\nIf you eschewed automated testing because it can't prove anything, you have no idea that the behavior of the system hasn't changed. In order to get the most confidence, you need to run a full regression on the entire application. This is expensive, prone to human error, and has a high level of risk.\n\nIf you embrace automated testing, you have a set of tests that validate the stability of behavior in your system and the ability to make large changes to your code base. This guarantees that the software behavior did not change. This is where the real value in automated testing lies.\n\n## The Extended Value Proposition\n\nAutomated tests don't check that your code is doing the right thing. They document your code in such a way that anybody can check that it continues to behave in the same way after some changes are made in the code. \n\nThis has some very interesting implications. The primary one is that **your tests specify exactly what your business does**. If you don't validate the consistency of behavior, that means the behavior is not important to your business.\n\nThis reason is why BDD, or Behavior Driven Development, has emerged as a technique to capture system behaviors at a very high level, often in text format. A developer or SDET then works with the business people to write some code to execute these behavior requirements automatically, and with every code change. With enough of these behavior scenarios, we end up with a living specification of your business. You could execute a re-platforming \\(moving from PHP to Java, for example\\) using these high-level behavior tests as a guide. Think about that for a minute. You could rebuild your software, and therefore your business, from these behavior specifications.\n\nA bunch of green lights on tests doesn't mean that your software is perfect. It means that your business is doing what you think it should be doing.\n\nThis behavior specification, then, is (arguably) more valuable than the software itself. It becomes a living document that formally describes your business, and lets you know when that behavior changes unexpectedly. That's powerful. It's also very, very valuable.  \n\n\n","n":0.028}}}]}
